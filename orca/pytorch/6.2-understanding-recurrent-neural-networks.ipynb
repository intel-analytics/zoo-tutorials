{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'1.7.1'"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding recurrent neural networks\n",
    "\n",
    "This notebook contains code migrated from samples found in Chapter 6, Section 2 of [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff). Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.\n",
    "\n",
    "\n",
    "## A first recurrent layer in PyTorch\n",
    "\n",
    "The process we just naively implemented in Numpy corresponds to an actual PyTorch layer: the `RNN` layer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.nn import RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is just one minor difference: `RNN` processes batches of sequences, not just a single sequence like in our Numpy example. This means that it takes inputs of shape `(seq_len, batch, input_size)`, rather than `(seq_len, input_size)`.\n",
    "\n",
    "Like all recurrent layers in PyTorh, `RNN` returns the full sequences of successive outputs for each time stamp (a 3D tensor of shape `(seq_len, batch, num_directions * hidden_size)`), and a tensor `h_n` of shape `(num_layers * num_directions, batch, hidden_size)`, which contains the hidden state for `t = seq_len`.  Let's take a look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Network1(\n  (embedding): Embedding(10000, 32)\n  (rnn): RNN(32, 32)\n)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Network1(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim):\n",
    "        super(Network1, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        output, hidden = self.rnn(embedded)\n",
    "\n",
    "        return hidden   # return only the last output for each input sequence\n",
    "\n",
    "INPUT_DIM = 10000\n",
    "EMBEDDING_DIM = 32\n",
    "HIDDEN_DIM = 32\n",
    "\n",
    "model1 = Network1(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "print(model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is sometimes useful to stack several recurrent layers one after the other in order to increase the representational power of a network. \n",
    "In such a setup, you have to get all intermediate layers to return full sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Network2(\n  (embedding): Embedding(10000, 32)\n  (rnn1): RNN(32, 32)\n  (rnn2): RNN(32, 32)\n  (rnn3): RNN(32, 32)\n  (rnn4): RNN(32, 32)\n)\n"
     ]
    }
   ],
   "source": [
    "class Network2(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim):\n",
    "        super(Network2, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.rnn1 = nn.RNN(embedding_dim, hidden_dim)\n",
    "        self.rnn2 = nn.RNN(embedding_dim, hidden_dim)\n",
    "        self.rnn3 = nn.RNN(embedding_dim, hidden_dim)\n",
    "        self.rnn4 = nn.RNN(embedding_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        output, hidden = self.rnn1(embedded)\n",
    "        output, hidden = self.rnn2(output)\n",
    "        output, hidden = self.rnn3(output)\n",
    "        output, hidden = self.rnn4(output)\n",
    "\n",
    "        return hidden\n",
    "\n",
    "\n",
    "model2 = Network2(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "print(model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to use such a model on the IMDB movie review classification problem. We will try to build a machine learning model to classify movie reviews into 2 categories: postive or negtive. The IMDB training set contains 25000 movie reviews labeled by sentiment (positive / negative). Since our model cannot process strings directly, we will use a preprocessed dataset `imdb.npz`, which has turned the words in the reviews into integers according to the frequency they appear in the dataset. For instance, the integer \"3\" encodes the 3rd most frequent word in the data. \n",
    "\n",
    "Before loading the dataset, let's first define some parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_char = 1          # The start of a sequence will be marked with 1.\n",
    "num_words = 10000       # Number of words to consider as features. 10000 most frequent words are kept.\n",
    "maxlen = 500            # Maximum sequence length. Cut texts after this number of words.\n",
    "index_from=3            # Index actual words with this index and higher.\n",
    "oov_char=2              # Words that were cut out because of the num_words limit will be replaced with 2.\n",
    "pad_char = 0            # Padding value.\n",
    "batch_size = 128\n",
    "seed = 113"
   ]
  },
  {
   "source": [
    "Now we can begin our data processing. First let's load the dataset:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset loading finished.\nLength of training set:  25000\n"
     ]
    }
   ],
   "source": [
    "from bigdl.dataset import base\n",
    "import numpy as np\n",
    "\n",
    "def download_imdb(dest_dir):\n",
    "    \"\"\"Download pre-processed IMDB movie review data\n",
    "\n",
    "    :argument\n",
    "        dest_dir: destination directory to store the data\n",
    "    :return\n",
    "        The absolute path of the stored data\n",
    "    \"\"\"\n",
    "    file_name = \"imdb.npz\"\n",
    "    file_abs_path = base.maybe_download(file_name,\n",
    "                                        dest_dir,\n",
    "                                        'https://s3.amazonaws.com/text-datasets/imdb.npz')\n",
    "    return file_abs_path\n",
    "\n",
    "def load_imdb(dest_dir='/tmp/.bigdl/dataset'):\n",
    "    \"\"\"Load IMDB dataset.\n",
    "\n",
    "    :argument\n",
    "        dest_dir: where to cache the data (relative to `~/.bigdl/dataset`).\n",
    "    :return\n",
    "        the train, test separated IMDB dataset.\n",
    "    \"\"\"\n",
    "    path = download_imdb(dest_dir)\n",
    "    f = np.load(path, allow_pickle=True)\n",
    "    x_train = f['x_train']\n",
    "    y_train = f['y_train']\n",
    "    x_test = f['x_test']\n",
    "    y_test = f['y_test']\n",
    "    f.close()\n",
    "\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = load_imdb(dest_dir='.data/.bigdl/dataset')\n",
    "print(\"Dataset loading finished.\")\n",
    "print(\"Length of training set: \", len(x_train))"
   ]
  },
  {
   "source": [
    "Since only the training set of IMDB is used in this example, we will only preprocess the training set. Shuffle the data:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(seed)\n",
    "indices = np.arange(len(x_train))\n",
    "rng.shuffle(indices)\n",
    "x_train = x_train[indices]\n",
    "y_train = y_train[indices]"
   ]
  },
  {
   "source": [
    "Set the start character:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [[start_char] + [w + index_from for w in x] for x in x_train]"
   ]
  },
  {
   "source": [
    "Since we only consider `num_words` words (features) in this example, any word out of this range will be disgarded. The disgarded words are represented as `oov_char`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [[w if (w < num_words) else oov_char for w in x] for x in x_train]"
   ]
  },
  {
   "source": [
    "When we feed sequences into our model, all sequences in the batch need to be the same size: `maxlen`. Thus, to ensure each sentence in the batch is the same size, any shorter than `maxlen` needs to be padded, and any longer needs to be cut. `pad_char` is used to fill the blanks in the shorter sequences."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(x_train)):\n",
    "    l = len(x_train[i])\n",
    "    if l >= maxlen:\n",
    "        x_train[i] = x_train[i][(l - maxlen):]\n",
    "    else:\n",
    "        x_train[i] =[pad_char] * (maxlen - l) + x_train[i]"
   ]
  },
  {
   "source": [
    "To create a validation set, we can split the the training set with 25000 samples into two parts: 20000 samples for training and 5000 samples for validation."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature, train_label = x_train[0:20000], y_train[0:20000]\n",
    "val_feature, val_label = x_train[20000:25000], y_train[20000:25000]"
   ]
  },
  {
   "source": [
    "Create data loaders for the training and validation datasets:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def create_data_loader(feature, label, batch_size, shuffle=True):\n",
    "    feature_tensor = torch.LongTensor(feature)\n",
    "    label_tensor = torch.FloatTensor(label).unsqueeze(1)\n",
    "    dataset = TensorDataset(feature_tensor, label_tensor)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "train_loader = create_data_loader(train_feature, train_label, batch_size)\n",
    "val_loader = create_data_loader(val_feature, val_label, batch_size, False)"
   ]
  },
  {
   "source": [
    "Now we are ready to create and train the model. First, initialize orca context:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Initializing orca context\n",
      "Current pyspark location is : /intern/spark/spark-2.4.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/__init__.py\n",
      "Start to getOrCreate SparkContext\n",
      "pyspark_submit_args is:  --driver-class-path /home/jinglei/analytics-zoo/zoo/target/analytics-zoo-bigdl_0.12.1-spark_2.4.3-0.10.0-SNAPSHOT-jar-with-dependencies.jar pyspark-shell \n",
      "2021-03-08 16:17:18 WARN  Utils:66 - Your hostname, intern01 resolves to a loopback address: 127.0.1.1; using 10.239.44.107 instead (on interface eno1)\n",
      "2021-03-08 16:17:18 WARN  Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "2021-03-08 16:17:18 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2021-03-08 16:17:19 WARN  Utils:66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/home/jinglei/analytics-zoo/zoo/target/analytics-zoo-bigdl_0.12.1-spark_2.4.3-0.10.0-SNAPSHOT-jar-with-dependencies.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/intern/spark/spark-2.4.3-bin-hadoop2.7/jars/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "cls.getname: com.intel.analytics.bigdl.python.api.Sample\n",
      "BigDLBasePickler registering: bigdl.util.common  Sample\n",
      "cls.getname: com.intel.analytics.bigdl.python.api.EvaluatedResult\n",
      "BigDLBasePickler registering: bigdl.util.common  EvaluatedResult\n",
      "cls.getname: com.intel.analytics.bigdl.python.api.JTensor\n",
      "BigDLBasePickler registering: bigdl.util.common  JTensor\n",
      "cls.getname: com.intel.analytics.bigdl.python.api.JActivity\n",
      "Successfully got a SparkContextBigDLBasePickler registering: bigdl.util.common  JActivity\n",
      "\n",
      "\n",
      "User settings:\n",
      "\n",
      "   KMP_AFFINITY=granularity=fine,compact,1,0\n",
      "   KMP_BLOCKTIME=0\n",
      "   KMP_SETTINGS=1\n",
      "   OMP_NUM_THREADS=1\n",
      "\n",
      "Effective settings:\n",
      "\n",
      "   KMP_ABORT_DELAY=0\n",
      "   KMP_ADAPTIVE_LOCK_PROPS='1,1024'\n",
      "   KMP_ALIGN_ALLOC=64\n",
      "   KMP_ALL_THREADPRIVATE=128\n",
      "   KMP_ATOMIC_MODE=2\n",
      "   KMP_BLOCKTIME=0\n",
      "   KMP_CPUINFO_FILE: value is not defined\n",
      "   KMP_DETERMINISTIC_REDUCTION=false\n",
      "   KMP_DEVICE_THREAD_LIMIT=2147483647\n",
      "   KMP_DISP_HAND_THREAD=false\n",
      "   KMP_DISP_NUM_BUFFERS=7\n",
      "   KMP_DUPLICATE_LIB_OK=false\n",
      "   KMP_FORCE_REDUCTION: value is not defined\n",
      "   KMP_FOREIGN_THREADS_THREADPRIVATE=true\n",
      "   KMP_FORKJOIN_BARRIER='2,2'\n",
      "   KMP_FORKJOIN_BARRIER_PATTERN='hyper,hyper'\n",
      "   KMP_FORKJOIN_FRAMES=true\n",
      "   KMP_FORKJOIN_FRAMES_MODE=3\n",
      "   KMP_GTID_MODE=3\n",
      "   KMP_HANDLE_SIGNALS=false\n",
      "   KMP_HOT_TEAMS_MAX_LEVEL=1\n",
      "   KMP_HOT_TEAMS_MODE=0\n",
      "   KMP_INIT_AT_FORK=true\n",
      "   KMP_INIT_WAIT=2048\n",
      "   KMP_ITT_PREPARE_DELAY=0\n",
      "   KMP_LIBRARY=throughput\n",
      "   KMP_LOCK_KIND=queuing\n",
      "   KMP_MALLOC_POOL_INCR=1M\n",
      "   KMP_NEXT_WAIT=1024\n",
      "   KMP_NUM_LOCKS_IN_BLOCK=1\n",
      "   KMP_PLAIN_BARRIER='2,2'\n",
      "   KMP_PLAIN_BARRIER_PATTERN='hyper,hyper'\n",
      "   KMP_REDUCTION_BARRIER='1,1'\n",
      "   KMP_REDUCTION_BARRIER_PATTERN='hyper,hyper'\n",
      "   KMP_SCHEDULE='static,balanced;guided,iterative'\n",
      "   KMP_SETTINGS=true\n",
      "   KMP_SPIN_BACKOFF_PARAMS='4096,100'\n",
      "   KMP_STACKOFFSET=64\n",
      "   KMP_STACKPAD=0\n",
      "   KMP_STACKSIZE=4M\n",
      "   KMP_STORAGE_MAP=false\n",
      "   KMP_TASKING=2\n",
      "   KMP_TASKLOOP_MIN_TASKS=0\n",
      "   KMP_TASK_STEALING_CONSTRAINT=1\n",
      "   KMP_TEAMS_THREAD_LIMIT=8\n",
      "   KMP_TOPOLOGY_METHOD=all\n",
      "   KMP_USER_LEVEL_MWAIT=false\n",
      "   KMP_VERSION=false\n",
      "   KMP_WARNINGS=true\n",
      "   OMP_AFFINITY_FORMAT='OMP: pid %P tid %T thread %n bound to OS proc set {%a}'\n",
      "   OMP_ALLOCATOR=omp_default_mem_alloc\n",
      "   OMP_CANCELLATION=false\n",
      "   OMP_DEFAULT_DEVICE=0\n",
      "   OMP_DISPLAY_AFFINITY=false\n",
      "   OMP_DISPLAY_ENV=false\n",
      "   OMP_DYNAMIC=false\n",
      "   OMP_MAX_ACTIVE_LEVELS=2147483647\n",
      "   OMP_MAX_TASK_PRIORITY=0\n",
      "   OMP_NESTED=false\n",
      "   OMP_NUM_THREADS='1'\n",
      "   OMP_PLACES: value is not defined\n",
      "   OMP_PROC_BIND='intel'\n",
      "   OMP_SCHEDULE='static'\n",
      "   OMP_STACKSIZE=4M\n",
      "   OMP_TARGET_OFFLOAD=DEFAULT\n",
      "   OMP_THREAD_LIMIT=2147483647\n",
      "   OMP_TOOL=enabled\n",
      "   OMP_TOOL_LIBRARIES: value is not defined\n",
      "   OMP_WAIT_POLICY=PASSIVE\n",
      "   KMP_AFFINITY='noverbose,warnings,respect,granularity=fine,compact,1,0'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from zoo.orca import init_orca_context, stop_orca_context\n",
    "from zoo.orca import OrcaContext\n",
    "\n",
    "# recommended to set it to True when running Analytics Zoo in Jupyter notebook. \n",
    "OrcaContext.log_output = True # (this will display terminal's stdout and stderr in the Jupyter notebook).\n",
    "\n",
    "cluster_mode = \"local\"\n",
    "\n",
    "if cluster_mode == \"local\":\n",
    "    init_orca_context(cores=1, memory=\"2g\")   # run in local mode\n",
    "elif cluster_mode == \"k8s\":\n",
    "    init_orca_context(cluster_mode=\"k8s\", num_nodes=2, cores=4) # run on K8s cluster\n",
    "elif cluster_mode == \"yarn\":\n",
    "    init_orca_context(\n",
    "        cluster_mode=\"yarn-client\", cores=4, num_nodes=2, memory=\"2g\",\n",
    "        driver_memory=\"10g\", driver_cores=1,\n",
    "        conf={\"spark.rpc.message.maxSize\": \"1024\",\n",
    "              \"spark.task.maxFailures\": \"1\",\n",
    "              \"spark.driver.extraJavaOptions\": \"-Dbigdl.failure.retryTimes=1\"})   # run on Hadoop YARN cluster"
   ]
  },
  {
   "source": [
    "Create a simple recurrent network using an Embedding layer and an RNN layer:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "SimpleRNN(\n  (embedding): Embedding(10000, 32)\n  (rnn): RNN(32, 32)\n  (fc): Linear(in_features=32, out_features=1, bias=True)\n  (out_act): Sigmoid()\n)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, 32)\n",
    "        for name, param in self.embedding.named_parameters(): \n",
    "            torch.nn.init.uniform_(param)\n",
    "        \n",
    "        self.rnn = nn.RNN(32, 32)\n",
    "        for name, param in self.rnn.named_parameters(): \n",
    "            if 'weight_ih' in name:\n",
    "                torch.nn.init.xavier_uniform_(param)\n",
    "            elif 'weight_hh' in name:\n",
    "                torch.nn.init.orthogonal_(param)\n",
    "            elif 'bias' in name:\n",
    "                torch.nn.init.zeros_(param)\n",
    "                \n",
    "        self.fc = nn.Linear(32, 1)\n",
    "        for name, param in self.rnn.named_parameters(): \n",
    "            if 'weight_ih' in name:\n",
    "                torch.nn.init.xavier_uniform_(param)\n",
    "            elif 'bias' in name:\n",
    "                torch.nn.init.zeros_(param)\n",
    "                \n",
    "        self.out_act = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text.transpose(0, 1))     # text: [128, 500] embedded: [500, 128, 32]\n",
    "        output, hidden = self.rnn(embedded)                 # hidden: [1, 128, 32]\n",
    "\n",
    "        es = \"output: \", output[-1,:,:].shape, \" hidden: \", hidden.shape\n",
    "        assert torch.equal(output[-1,:,:], hidden.squeeze(0)), es\n",
    "        \n",
    "        ret = self.fc(hidden.squeeze(0))                    # [128, 1]\n",
    "        #return ret.squeeze(1)\n",
    "        return self.out_act(ret)\n",
    "\n",
    "INPUT_DIM = 10000\n",
    "EMBEDDING_DIM = 32\n",
    "HIDDEN_DIM = 32\n",
    "\n",
    "model = SimpleRNN(INPUT_DIM)\n",
    "model.train()\n",
    "print(model)"
   ]
  },
  {
   "source": [
    "Specify loss function, optimizer and metrics:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zoo.orca.learn.metrics import Accuracy\n",
    "\n",
    "rmsprop = torch.optim.RMSprop(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.BCELoss()\n",
    "metrics=[Accuracy()]"
   ]
  },
  {
   "source": [
    "Now we can start the training:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Epoch 3 9472/20096][Iteration 388][Wall Clock 138.068877742s] Trained 128.0 records in 0.275711347 seconds. Throughput is 464.25363 records/second. Loss is 0.4504674. \n",
      "2021-03-08 16:20:48 INFO  DistriOptimizer$:427 - [Epoch 3 9600/20096][Iteration 389][Wall Clock 138.426206647s] Trained 128.0 records in 0.357328905 seconds. Throughput is 358.2134 records/second. Loss is 0.5536856. \n",
      "2021-03-08 16:20:48 INFO  DistriOptimizer$:427 - [Epoch 3 9728/20096][Iteration 390][Wall Clock 138.740832851s] Trained 128.0 records in 0.314626204 seconds. Throughput is 406.83197 records/second. Loss is 0.60600984. \n",
      "2021-03-08 16:20:48 INFO  DistriOptimizer$:427 - [Epoch 3 9856/20096][Iteration 391][Wall Clock 139.016926222s] Trained 128.0 records in 0.276093371 seconds. Throughput is 463.6113 records/second. Loss is 0.49867636. \n",
      "2021-03-08 16:20:48 INFO  DistriOptimizer$:427 - [Epoch 3 9984/20096][Iteration 392][Wall Clock 139.251754281s] Trained 128.0 records in 0.234828059 seconds. Throughput is 545.07965 records/second. Loss is 0.48257723. \n",
      "2021-03-08 16:20:49 INFO  DistriOptimizer$:427 - [Epoch 3 10112/20096][Iteration 393][Wall Clock 139.521659492s] Trained 128.0 records in 0.269905211 seconds. Throughput is 474.24057 records/second. Loss is 0.44951493. \n",
      "2021-03-08 16:20:49 INFO  DistriOptimizer$:427 - [Epoch 3 10240/20096][Iteration 394][Wall Clock 139.850725046s] Trained 128.0 records in 0.329065554 seconds. Throughput is 388.98022 records/second. Loss is 0.54276866. \n",
      "2021-03-08 16:20:49 INFO  DistriOptimizer$:427 - [Epoch 3 10368/20096][Iteration 395][Wall Clock 140.090726909s] Trained 128.0 records in 0.240001863 seconds. Throughput is 533.3292 records/second. Loss is 0.45262504. \n",
      "2021-03-08 16:20:50 INFO  DistriOptimizer$:427 - [Epoch 3 10496/20096][Iteration 396][Wall Clock 140.326824534s] Trained 128.0 records in 0.236097625 seconds. Throughput is 542.1486 records/second. Loss is 0.46853563. \n",
      "2021-03-08 16:20:50 INFO  DistriOptimizer$:427 - [Epoch 3 10624/20096][Iteration 397][Wall Clock 140.600523086s] Trained 128.0 records in 0.273698552 seconds. Throughput is 467.66782 records/second. Loss is 0.44651088. \n",
      "2021-03-08 16:20:50 INFO  DistriOptimizer$:427 - [Epoch 3 10752/20096][Iteration 398][Wall Clock 140.897303632s] Trained 128.0 records in 0.296780546 seconds. Throughput is 431.2951 records/second. Loss is 0.52064145. \n",
      "2021-03-08 16:20:50 INFO  DistriOptimizer$:427 - [Epoch 3 10880/20096][Iteration 399][Wall Clock 141.238311358s] Trained 128.0 records in 0.341007726 seconds. Throughput is 375.35806 records/second. Loss is 0.49568096. \n",
      "2021-03-08 16:20:51 INFO  DistriOptimizer$:427 - [Epoch 3 11008/20096][Iteration 400][Wall Clock 141.502292716s] Trained 128.0 records in 0.263981358 seconds. Throughput is 484.8827 records/second. Loss is 0.5048681. \n",
      "2021-03-08 16:20:51 INFO  DistriOptimizer$:427 - [Epoch 3 11136/20096][Iteration 401][Wall Clock 141.804850904s] Trained 128.0 records in 0.302558188 seconds. Throughput is 423.0591 records/second. Loss is 0.46553. \n",
      "2021-03-08 16:20:51 INFO  DistriOptimizer$:427 - [Epoch 3 11264/20096][Iteration 402][Wall Clock 142.127202825s] Trained 128.0 records in 0.322351921 seconds. Throughput is 397.08154 records/second. Loss is 0.649403. \n",
      "2021-03-08 16:20:52 INFO  DistriOptimizer$:427 - [Epoch 3 11392/20096][Iteration 403][Wall Clock 142.406869571s] Trained 128.0 records in 0.279666746 seconds. Throughput is 457.6876 records/second. Loss is 0.6917087. \n",
      "2021-03-08 16:20:52 INFO  DistriOptimizer$:427 - [Epoch 3 11520/20096][Iteration 404][Wall Clock 142.600415389s] Trained 128.0 records in 0.193545818 seconds. Throughput is 661.3421 records/second. Loss is 0.4742557. \n",
      "2021-03-08 16:20:52 INFO  DistriOptimizer$:427 - [Epoch 3 11648/20096][Iteration 405][Wall Clock 142.823628745s] Trained 128.0 records in 0.223213356 seconds. Throughput is 573.4424 records/second. Loss is 0.54244095. \n",
      "2021-03-08 16:20:52 INFO  DistriOptimizer$:427 - [Epoch 3 11776/20096][Iteration 406][Wall Clock 143.03425689s] Trained 128.0 records in 0.210628145 seconds. Throughput is 607.70605 records/second. Loss is 0.46258402. \n",
      "2021-03-08 16:20:52 INFO  DistriOptimizer$:427 - [Epoch 3 11904/20096][Iteration 407][Wall Clock 143.259597567s] Trained 128.0 records in 0.225340677 seconds. Throughput is 568.0288 records/second. Loss is 0.5401781. \n",
      "2021-03-08 16:20:53 INFO  DistriOptimizer$:427 - [Epoch 3 12032/20096][Iteration 408][Wall Clock 143.508332095s] Trained 128.0 records in 0.248734528 seconds. Throughput is 514.60486 records/second. Loss is 0.53820324. \n",
      "2021-03-08 16:20:53 INFO  DistriOptimizer$:427 - [Epoch 3 12160/20096][Iteration 409][Wall Clock 143.796415044s] Trained 128.0 records in 0.288082949 seconds. Throughput is 444.31647 records/second. Loss is 0.44375098. \n",
      "2021-03-08 16:20:53 INFO  DistriOptimizer$:427 - [Epoch 3 12288/20096][Iteration 410][Wall Clock 144.083643301s] Trained 128.0 records in 0.287228257 seconds. Throughput is 445.6386 records/second. Loss is 0.6023656. \n",
      "2021-03-08 16:20:54 INFO  DistriOptimizer$:427 - [Epoch 3 12416/20096][Iteration 411][Wall Clock 144.44020426s] Trained 128.0 records in 0.356560959 seconds. Throughput is 358.98492 records/second. Loss is 0.5810809. \n",
      "2021-03-08 16:20:54 INFO  DistriOptimizer$:427 - [Epoch 3 12544/20096][Iteration 412][Wall Clock 144.662776092s] Trained 128.0 records in 0.222571832 seconds. Throughput is 575.0952 records/second. Loss is 0.6117796. \n",
      "2021-03-08 16:20:54 INFO  DistriOptimizer$:427 - [Epoch 3 12672/20096][Iteration 413][Wall Clock 144.937071574s] Trained 128.0 records in 0.274295482 seconds. Throughput is 466.65005 records/second. Loss is 0.53709364. \n",
      "2021-03-08 16:20:54 INFO  DistriOptimizer$:427 - [Epoch 3 12800/20096][Iteration 414][Wall Clock 145.232967078s] Trained 128.0 records in 0.295895504 seconds. Throughput is 432.5851 records/second. Loss is 0.4203291. \n",
      "2021-03-08 16:20:55 INFO  DistriOptimizer$:427 - [Epoch 3 12928/20096][Iteration 415][Wall Clock 145.5663534s] Trained 128.0 records in 0.333386322 seconds. Throughput is 383.93896 records/second. Loss is 0.56853354. \n",
      "2021-03-08 16:20:55 INFO  DistriOptimizer$:427 - [Epoch 3 13056/20096][Iteration 416][Wall Clock 145.876059564s] Trained 128.0 records in 0.309706164 seconds. Throughput is 413.29498 records/second. Loss is 0.50247324. \n",
      "2021-03-08 16:20:55 INFO  DistriOptimizer$:427 - [Epoch 3 13184/20096][Iteration 417][Wall Clock 146.190325242s] Trained 128.0 records in 0.314265678 seconds. Throughput is 407.2987 records/second. Loss is 0.45779705. \n",
      "2021-03-08 16:20:56 INFO  DistriOptimizer$:427 - [Epoch 3 13312/20096][Iteration 418][Wall Clock 146.473406087s] Trained 128.0 records in 0.283080845 seconds. Throughput is 452.16763 records/second. Loss is 0.49910587. \n",
      "2021-03-08 16:20:56 INFO  DistriOptimizer$:427 - [Epoch 3 13440/20096][Iteration 419][Wall Clock 146.760988053s] Trained 128.0 records in 0.287581966 seconds. Throughput is 445.09048 records/second. Loss is 0.54004407. \n",
      "2021-03-08 16:20:56 INFO  DistriOptimizer$:427 - [Epoch 3 13568/20096][Iteration 420][Wall Clock 147.076283118s] Trained 128.0 records in 0.315295065 seconds. Throughput is 405.96893 records/second. Loss is 0.6330019. \n",
      "2021-03-08 16:20:57 INFO  DistriOptimizer$:427 - [Epoch 3 13696/20096][Iteration 421][Wall Clock 147.274074602s] Trained 128.0 records in 0.197791484 seconds. Throughput is 647.1462 records/second. Loss is 0.50818956. \n",
      "2021-03-08 16:20:57 INFO  DistriOptimizer$:427 - [Epoch 3 13824/20096][Iteration 422][Wall Clock 147.510137272s] Trained 128.0 records in 0.23606267 seconds. Throughput is 542.2289 records/second. Loss is 0.49352795. \n",
      "2021-03-08 16:20:57 INFO  DistriOptimizer$:427 - [Epoch 3 13952/20096][Iteration 423][Wall Clock 147.781021387s] Trained 128.0 records in 0.270884115 seconds. Throughput is 472.52676 records/second. Loss is 0.5238475. \n",
      "2021-03-08 16:20:57 INFO  DistriOptimizer$:427 - [Epoch 3 14080/20096][Iteration 424][Wall Clock 148.088432199s] Trained 128.0 records in 0.307410812 seconds. Throughput is 416.38095 records/second. Loss is 0.5261426. \n",
      "2021-03-08 16:20:58 INFO  DistriOptimizer$:427 - [Epoch 3 14208/20096][Iteration 425][Wall Clock 148.37928476s] Trained 128.0 records in 0.290852561 seconds. Throughput is 440.08554 records/second. Loss is 0.51482475. \n",
      "2021-03-08 16:20:58 INFO  DistriOptimizer$:427 - [Epoch 3 14336/20096][Iteration 426][Wall Clock 148.704399019s] Trained 128.0 records in 0.325114259 seconds. Throughput is 393.70776 records/second. Loss is 0.45210004. \n",
      "2021-03-08 16:20:58 INFO  DistriOptimizer$:427 - [Epoch 3 14464/20096][Iteration 427][Wall Clock 149.058536252s] Trained 128.0 records in 0.354137233 seconds. Throughput is 361.4418 records/second. Loss is 0.49153137. \n",
      "2021-03-08 16:20:59 INFO  DistriOptimizer$:427 - [Epoch 3 14592/20096][Iteration 428][Wall Clock 149.416973798s] Trained 128.0 records in 0.358437546 seconds. Throughput is 357.10547 records/second. Loss is 0.71813387. \n",
      "2021-03-08 16:20:59 INFO  DistriOptimizer$:427 - [Epoch 3 14720/20096][Iteration 429][Wall Clock 149.716971912s] Trained 128.0 records in 0.299998114 seconds. Throughput is 426.66937 records/second. Loss is 0.48860157. \n",
      "2021-03-08 16:20:59 INFO  DistriOptimizer$:427 - [Epoch 3 14848/20096][Iteration 430][Wall Clock 149.950728977s] Trained 128.0 records in 0.233757065 seconds. Throughput is 547.577 records/second. Loss is 0.52788776. \n",
      "2021-03-08 16:20:59 INFO  DistriOptimizer$:427 - [Epoch 3 14976/20096][Iteration 431][Wall Clock 150.18932428s] Trained 128.0 records in 0.238595303 seconds. Throughput is 536.47327 records/second. Loss is 0.50506884. \n",
      "2021-03-08 16:21:00 INFO  DistriOptimizer$:427 - [Epoch 3 15104/20096][Iteration 432][Wall Clock 150.429740551s] Trained 128.0 records in 0.240416271 seconds. Throughput is 532.4099 records/second. Loss is 0.4569624. \n",
      "2021-03-08 16:21:00 INFO  DistriOptimizer$:427 - [Epoch 3 15232/20096][Iteration 433][Wall Clock 150.676733931s] Trained 128.0 records in 0.24699338 seconds. Throughput is 518.23254 records/second. Loss is 0.5337797. \n",
      "2021-03-08 16:21:00 INFO  DistriOptimizer$:427 - [Epoch 3 15360/20096][Iteration 434][Wall Clock 151.057910343s] Trained 128.0 records in 0.381176412 seconds. Throughput is 335.80252 records/second. Loss is 0.6274249. \n",
      "2021-03-08 16:21:00 INFO  DistriOptimizer$:427 - [Epoch 3 15488/20096][Iteration 435][Wall Clock 151.262756737s] Trained 128.0 records in 0.204846394 seconds. Throughput is 624.85846 records/second. Loss is 0.47600245. \n",
      "2021-03-08 16:21:01 INFO  DistriOptimizer$:427 - [Epoch 3 15616/20096][Iteration 436][Wall Clock 151.504920164s] Trained 128.0 records in 0.242163427 seconds. Throughput is 528.56866 records/second. Loss is 0.43766946. \n",
      "2021-03-08 16:21:01 INFO  DistriOptimizer$:427 - [Epoch 3 15744/20096][Iteration 437][Wall Clock 151.866141355s] Trained 128.0 records in 0.361221191 seconds. Throughput is 354.35352 records/second. Loss is 0.43231776. \n",
      "2021-03-08 16:21:01 INFO  DistriOptimizer$:427 - [Epoch 3 15872/20096][Iteration 438][Wall Clock 152.227576319s] Trained 128.0 records in 0.361434964 seconds. Throughput is 354.14392 records/second. Loss is 0.5134203. \n",
      "2021-03-08 16:21:02 INFO  DistriOptimizer$:427 - [Epoch 3 16000/20096][Iteration 439][Wall Clock 152.599094617s] Trained 128.0 records in 0.371518298 seconds. Throughput is 344.53217 records/second. Loss is 0.48069215. \n",
      "2021-03-08 16:21:02 INFO  DistriOptimizer$:427 - [Epoch 3 16128/20096][Iteration 440][Wall Clock 152.851959098s] Trained 128.0 records in 0.252864481 seconds. Throughput is 506.2 records/second. Loss is 0.4552472. \n",
      "2021-03-08 16:21:02 INFO  DistriOptimizer$:427 - [Epoch 3 16256/20096][Iteration 441][Wall Clock 153.189581615s] Trained 128.0 records in 0.337622517 seconds. Throughput is 379.1216 records/second. Loss is 0.47978857. \n",
      "2021-03-08 16:21:03 INFO  DistriOptimizer$:427 - [Epoch 3 16384/20096][Iteration 442][Wall Clock 153.43159177s] Trained 128.0 records in 0.242010155 seconds. Throughput is 528.9034 records/second. Loss is 0.5695108. \n",
      "2021-03-08 16:21:03 INFO  DistriOptimizer$:427 - [Epoch 3 16512/20096][Iteration 443][Wall Clock 153.70407923s] Trained 128.0 records in 0.27248746 seconds. Throughput is 469.7464 records/second. Loss is 0.53757036. \n",
      "2021-03-08 16:21:03 INFO  DistriOptimizer$:427 - [Epoch 3 16640/20096][Iteration 444][Wall Clock 153.921913089s] Trained 128.0 records in 0.217833859 seconds. Throughput is 587.60376 records/second. Loss is 0.50048596. \n",
      "2021-03-08 16:21:03 INFO  DistriOptimizer$:427 - [Epoch 3 16768/20096][Iteration 445][Wall Clock 154.225835411s] Trained 128.0 records in 0.303922322 seconds. Throughput is 421.16025 records/second. Loss is 0.5061941. \n",
      "2021-03-08 16:21:04 INFO  DistriOptimizer$:427 - [Epoch 3 16896/20096][Iteration 446][Wall Clock 154.427327355s] Trained 128.0 records in 0.201491944 seconds. Throughput is 635.26117 records/second. Loss is 0.47966298. \n",
      "2021-03-08 16:21:04 INFO  DistriOptimizer$:427 - [Epoch 3 17024/20096][Iteration 447][Wall Clock 154.646421552s] Trained 128.0 records in 0.219094197 seconds. Throughput is 584.2236 records/second. Loss is 0.47600895. \n",
      "2021-03-08 16:21:04 INFO  DistriOptimizer$:427 - [Epoch 3 17152/20096][Iteration 448][Wall Clock 154.902736793s] Trained 128.0 records in 0.256315241 seconds. Throughput is 499.38507 records/second. Loss is 0.50017434. \n",
      "2021-03-08 16:21:04 INFO  DistriOptimizer$:427 - [Epoch 3 17280/20096][Iteration 449][Wall Clock 155.165708958s] Trained 128.0 records in 0.262972165 seconds. Throughput is 486.7435 records/second. Loss is 0.5998494. \n",
      "2021-03-08 16:21:05 INFO  DistriOptimizer$:427 - [Epoch 3 17408/20096][Iteration 450][Wall Clock 155.565184962s] Trained 128.0 records in 0.399476004 seconds. Throughput is 320.41977 records/second. Loss is 0.8953105. \n",
      "2021-03-08 16:21:05 INFO  DistriOptimizer$:427 - [Epoch 3 17536/20096][Iteration 451][Wall Clock 155.96758584s] Trained 128.0 records in 0.402400878 seconds. Throughput is 318.09076 records/second. Loss is 0.54004455. \n",
      "2021-03-08 16:21:06 INFO  DistriOptimizer$:427 - [Epoch 3 17664/20096][Iteration 452][Wall Clock 156.308683324s] Trained 128.0 records in 0.341097484 seconds. Throughput is 375.2593 records/second. Loss is 0.51855934. \n",
      "2021-03-08 16:21:06 INFO  DistriOptimizer$:427 - [Epoch 3 17792/20096][Iteration 453][Wall Clock 156.636641604s] Trained 128.0 records in 0.32795828 seconds. Throughput is 390.29355 records/second. Loss is 0.44788602. \n",
      "2021-03-08 16:21:06 INFO  DistriOptimizer$:427 - [Epoch 3 17920/20096][Iteration 454][Wall Clock 156.966703383s] Trained 128.0 records in 0.330061779 seconds. Throughput is 387.80618 records/second. Loss is 0.4896536. \n",
      "2021-03-08 16:21:07 INFO  DistriOptimizer$:427 - [Epoch 3 18048/20096][Iteration 455][Wall Clock 157.356701636s] Trained 128.0 records in 0.389998253 seconds. Throughput is 328.2066 records/second. Loss is 0.52650297. \n",
      "2021-03-08 16:21:07 INFO  DistriOptimizer$:427 - [Epoch 3 18176/20096][Iteration 456][Wall Clock 157.708216989s] Trained 128.0 records in 0.351515353 seconds. Throughput is 364.13773 records/second. Loss is 0.62115794. \n",
      "2021-03-08 16:21:07 INFO  DistriOptimizer$:427 - [Epoch 3 18304/20096][Iteration 457][Wall Clock 157.95887825s] Trained 128.0 records in 0.250661261 seconds. Throughput is 510.64932 records/second. Loss is 0.46712673. \n",
      "2021-03-08 16:21:07 INFO  DistriOptimizer$:427 - [Epoch 3 18432/20096][Iteration 458][Wall Clock 158.210505084s] Trained 128.0 records in 0.251626834 seconds. Throughput is 508.68982 records/second. Loss is 0.5218579. \n",
      "2021-03-08 16:21:08 INFO  DistriOptimizer$:427 - [Epoch 3 18560/20096][Iteration 459][Wall Clock 158.491444111s] Trained 128.0 records in 0.280939027 seconds. Throughput is 455.6149 records/second. Loss is 0.53730595. \n",
      "2021-03-08 16:21:08 INFO  DistriOptimizer$:427 - [Epoch 3 18688/20096][Iteration 460][Wall Clock 158.688550582s] Trained 128.0 records in 0.197106471 seconds. Throughput is 649.39526 records/second. Loss is 0.45664406. \n",
      "2021-03-08 16:21:08 INFO  DistriOptimizer$:427 - [Epoch 3 18816/20096][Iteration 461][Wall Clock 158.956593034s] Trained 128.0 records in 0.268042452 seconds. Throughput is 477.53632 records/second. Loss is 0.5201442. \n",
      "2021-03-08 16:21:08 INFO  DistriOptimizer$:427 - [Epoch 3 18944/20096][Iteration 462][Wall Clock 159.160316328s] Trained 128.0 records in 0.203723294 seconds. Throughput is 628.3032 records/second. Loss is 0.5232432. \n",
      "2021-03-08 16:21:09 INFO  DistriOptimizer$:427 - [Epoch 3 19072/20096][Iteration 463][Wall Clock 159.472354783s] Trained 128.0 records in 0.312038455 seconds. Throughput is 410.20584 records/second. Loss is 0.5097176. \n",
      "2021-03-08 16:21:09 INFO  DistriOptimizer$:427 - [Epoch 3 19200/20096][Iteration 464][Wall Clock 159.679991586s] Trained 128.0 records in 0.207636803 seconds. Throughput is 616.46106 records/second. Loss is 0.48863903. \n",
      "2021-03-08 16:21:09 INFO  DistriOptimizer$:427 - [Epoch 3 19328/20096][Iteration 465][Wall Clock 159.905466021s] Trained 128.0 records in 0.225474435 seconds. Throughput is 567.69183 records/second. Loss is 0.47866192. \n",
      "2021-03-08 16:21:09 INFO  DistriOptimizer$:427 - [Epoch 3 19456/20096][Iteration 466][Wall Clock 160.192606584s] Trained 128.0 records in 0.287140563 seconds. Throughput is 445.77472 records/second. Loss is 0.47549325. \n",
      "2021-03-08 16:21:10 INFO  DistriOptimizer$:427 - [Epoch 3 19584/20096][Iteration 467][Wall Clock 160.478203037s] Trained 128.0 records in 0.285596453 seconds. Throughput is 448.1848 records/second. Loss is 0.41619375. \n",
      "2021-03-08 16:21:10 INFO  DistriOptimizer$:427 - [Epoch 3 19712/20096][Iteration 468][Wall Clock 160.768099431s] Trained 128.0 records in 0.289896394 seconds. Throughput is 441.53705 records/second. Loss is 0.49321508. \n",
      "2021-03-08 16:21:10 INFO  DistriOptimizer$:427 - [Epoch 3 19840/20096][Iteration 469][Wall Clock 161.075157037s] Trained 128.0 records in 0.307057606 seconds. Throughput is 416.85986 records/second. Loss is 0.4094777. \n",
      "2021-03-08 16:21:11 INFO  DistriOptimizer$:427 - [Epoch 3 19968/20096][Iteration 470][Wall Clock 161.40850029s] Trained 128.0 records in 0.333343253 seconds. Throughput is 383.98856 records/second. Loss is 0.49730828. \n",
      "2021-03-08 16:21:11 INFO  DistriOptimizer$:427 - [Epoch 3 20096/20096][Iteration 471][Wall Clock 161.499545213s] Trained 128.0 records in 0.091044923 seconds. Throughput is 1405.8993 records/second. Loss is 0.7059927. \n",
      "2021-03-08 16:21:11 INFO  DistriOptimizer$:472 - [Epoch 3 20096/20096][Iteration 471][Wall Clock 161.499545213s] Epoch finished. Wall clock time is 163230.604775 ms\n",
      "2021-03-08 16:21:11 INFO  DistriOptimizer$:111 - [Epoch 3 20096/20096][Iteration 471][Wall Clock 161.499545213s] Validate model...\n",
      "[Stage 953:>                                                        (0 + 1) / 1]Warn: jep.JepException: <class 'StopIteration'>\n",
      "\tat jep.Jep.exec(Native Method)\n",
      "\tat jep.Jep.exec(Jep.java:478)\n",
      "\tat com.intel.analytics.zoo.common.PythonInterpreter$$anonfun$1.apply$mcV$sp(PythonInterpreter.scala:108)\n",
      "\tat com.intel.analytics.zoo.common.PythonInterpreter$$anonfun$1.apply(PythonInterpreter.scala:107)\n",
      "\tat com.intel.analytics.zoo.common.PythonInterpreter$$anonfun$1.apply(PythonInterpreter.scala:107)\n",
      "\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n",
      "\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2021-03-08 16:21:12 INFO  DistriOptimizer$:177 - [Epoch 3 20096/20096][Iteration 471][Wall Clock 161.499545213s] validate model throughput is 3156.43 records/second\n",
      "2021-03-08 16:21:12 INFO  DistriOptimizer$:180 - [Epoch 3 20096/20096][Iteration 471][Wall Clock 161.499545213s] Top1Accuracy is Accuracy(correct: 3481, count: 5000, accuracy: 0.6962)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<zoo.orca.learn.pytorch.estimator.PyTorchSparkEstimator at 0x7feac80d0c10>"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "from zoo.orca.learn.pytorch import Estimator\n",
    "from zoo.orca.learn.trigger import EveryEpoch\n",
    "\n",
    "est = Estimator.from_torch(model=model, optimizer=rmsprop, loss=criterion, metrics=metrics)\n",
    "est.fit(data=train_loader, epochs=3, validation_data=val_loader, batch_size=batch_size, checkpoint_trigger=EveryEpoch())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Stopping orca context\n"
     ]
    }
   ],
   "source": [
    "stop_orca_context()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('tf2': conda)",
   "metadata": {
    "interpreter": {
     "hash": "5056d60bd69d51d1e2df40a6c6e7c71a949f2ed4ad3fa2ff8fce432a7c7c0fe5"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}