{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'1.7.1'"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding recurrent neural networks\n",
    "\n",
    "This notebook contains code migrated from samples found in Chapter 6, Section 2 of [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff). Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.\n",
    "\n",
    "\n",
    "## A first recurrent layer in PyTorch\n",
    "\n",
    "The process we just naively implemented in Numpy corresponds to an actual PyTorch layer: the `RNN` layer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.nn import RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is just one minor difference: `RNN` processes batches of sequences, not just a single sequence like in our Numpy example. This means that it takes inputs of shape `(seq_len, batch, input_size)`, rather than `(seq_len, input_size)`.\n",
    "\n",
    "Like all recurrent layers in PyTorh, `RNN` returns the full sequences of successive outputs for each time stamp (a 3D tensor of shape `(seq_len, batch, num_directions * hidden_size)`), and a tensor `h_n` of shape `(num_layers * num_directions, batch, hidden_size)`, which contains the hidden state for `t = seq_len`.  Let's take a look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Network1(\n  (embedding): Embedding(10000, 32)\n  (rnn): RNN(32, 32)\n)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Network1(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim):\n",
    "        super(Network1, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        output, hidden = self.rnn(embedded)\n",
    "\n",
    "        return hidden   # return only the last output for each input sequence\n",
    "\n",
    "INPUT_DIM = 10000\n",
    "EMBEDDING_DIM = 32\n",
    "HIDDEN_DIM = 32\n",
    "\n",
    "model1 = Network1(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "print(model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is sometimes useful to stack several recurrent layers one after the other in order to increase the representational power of a network. \n",
    "In such a setup, you have to get all intermediate layers to return full sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Network2(\n  (embedding): Embedding(10000, 32)\n  (rnn1): RNN(32, 32)\n  (rnn2): RNN(32, 32)\n  (rnn3): RNN(32, 32)\n  (rnn4): RNN(32, 32)\n)\n"
     ]
    }
   ],
   "source": [
    "class Network2(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim):\n",
    "        super(Network2, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.rnn1 = nn.RNN(embedding_dim, hidden_dim)\n",
    "        self.rnn2 = nn.RNN(embedding_dim, hidden_dim)\n",
    "        self.rnn3 = nn.RNN(embedding_dim, hidden_dim)\n",
    "        self.rnn4 = nn.RNN(embedding_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        output, hidden = self.rnn1(embedded)\n",
    "        output, hidden = self.rnn2(output)\n",
    "        output, hidden = self.rnn3(output)\n",
    "        output, hidden = self.rnn4(output)\n",
    "\n",
    "        return hidden\n",
    "\n",
    "\n",
    "model2 = Network2(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "print(model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to use such a model on the IMDB movie review classification problem. We will try to build a machine learning model to classify movie reviews into 2 categories: postive or negtive. The IMDB training set contains 25000 movie reviews labeled by sentiment (positive / negative). Since our model cannot process strings directly, we will use a preprocessed dataset `imdb.npz`, which has turned the words in the reviews into integers according to the frequency they appear in the dataset. For instance, the integer \"3\" encodes the 3rd most frequent word in the data. \n",
    "\n",
    "Before loading the dataset, let's first define some parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_char = 1          # The start of a sequence will be marked with 1.\n",
    "num_words = 10000       # Number of words to consider as features. 10000 most frequent words are kept.\n",
    "maxlen = 500            # Maximum sequence length. Cut texts after this number of words.\n",
    "index_from=3            # Index actual words with this index and higher.\n",
    "oov_char=2              # Words that were cut out because of the num_words limit will be replaced with 2.\n",
    "pad_char = 0            # Padding value.\n",
    "batch_size = 128\n",
    "seed = 113"
   ]
  },
  {
   "source": [
    "Now we can begin our data processing. First let's load the dataset:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset loading finished.\nLength of training set:  25000\n"
     ]
    }
   ],
   "source": [
    "from bigdl.dataset import base\n",
    "import numpy as np\n",
    "\n",
    "def download_imdb(dest_dir):\n",
    "    \"\"\"Download pre-processed IMDB movie review data\n",
    "\n",
    "    :argument\n",
    "        dest_dir: destination directory to store the data\n",
    "    :return\n",
    "        The absolute path of the stored data\n",
    "    \"\"\"\n",
    "    file_name = \"imdb.npz\"\n",
    "    file_abs_path = base.maybe_download(file_name,\n",
    "                                        dest_dir,\n",
    "                                        'https://s3.amazonaws.com/text-datasets/imdb.npz')\n",
    "    return file_abs_path\n",
    "\n",
    "def load_imdb(dest_dir='/tmp/.bigdl/dataset'):\n",
    "    \"\"\"Load IMDB dataset.\n",
    "\n",
    "    :argument\n",
    "        dest_dir: where to cache the data (relative to `~/.bigdl/dataset`).\n",
    "    :return\n",
    "        the train, test separated IMDB dataset.\n",
    "    \"\"\"\n",
    "    path = download_imdb(dest_dir)\n",
    "    f = np.load(path, allow_pickle=True)\n",
    "    x_train = f['x_train']\n",
    "    y_train = f['y_train']\n",
    "    x_test = f['x_test']\n",
    "    y_test = f['y_test']\n",
    "    f.close()\n",
    "\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = load_imdb(dest_dir='.data/.bigdl/dataset')\n",
    "print(\"Dataset loading finished.\")\n",
    "print(\"Length of training set: \", len(x_train))"
   ]
  },
  {
   "source": [
    "Since only the training set of IMDB is used in this example, we will only preprocess the training set. Shuffle the data:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(seed)\n",
    "indices = np.arange(len(x_train))\n",
    "rng.shuffle(indices)\n",
    "x_train = x_train[indices]\n",
    "y_train = y_train[indices]"
   ]
  },
  {
   "source": [
    "Set the start character:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [[start_char] + [w + index_from for w in x] for x in x_train]"
   ]
  },
  {
   "source": [
    "Since we only consider `num_words` words (features) in this example, any word out of this range will be disgarded. The disgarded words are represented as `oov_char`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [[w if (w < num_words) else oov_char for w in x] for x in x_train]"
   ]
  },
  {
   "source": [
    "When we feed sequences into our model, all sequences in the batch need to be the same size: `maxlen`. Thus, to ensure each sentence in the batch is the same size, any shorter than `maxlen` needs to be padded, and any longer needs to be cut. `pad_char` is used to fill the blanks in the shorter sequences."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(x_train)):\n",
    "    l = len(x_train[i])\n",
    "    if l >= maxlen:\n",
    "        x_train[i] = x_train[i][(l - maxlen):]\n",
    "    else:\n",
    "        x_train[i] =[pad_char] * (maxlen - l) + x_train[i]"
   ]
  },
  {
   "source": [
    "To create a validation set, we can split the the training set with 25000 samples into two parts: 20000 samples for training and 5000 samples for validation."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature, train_label = x_train[0:20000], y_train[0:20000]\n",
    "val_feature, val_label = x_train[20000:25000], y_train[20000:25000]"
   ]
  },
  {
   "source": [
    "Create data loaders for the training and validation datasets:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def create_data_loader(feature, label, batch_size, shuffle=True):\n",
    "    feature_tensor = torch.LongTensor(feature)\n",
    "    label_tensor = torch.FloatTensor(label).unsqueeze(1)\n",
    "    dataset = TensorDataset(feature_tensor, label_tensor)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "train_loader = create_data_loader(train_feature, train_label, batch_size)\n",
    "val_loader = create_data_loader(val_feature, val_label, batch_size, False)"
   ]
  },
  {
   "source": [
    "Now we are ready to create and train the model. First, initialize orca context:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Initializing orca context\n",
      "Current pyspark location is : /intern/spark/spark-2.4.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/__init__.py\n",
      "Start to getOrCreate SparkContext\n",
      "pyspark_submit_args is:  --driver-class-path /home/jinglei/analytics-zoo/zoo/target/analytics-zoo-bigdl_0.12.1-spark_2.4.3-0.10.0-SNAPSHOT-jar-with-dependencies.jar pyspark-shell \n",
      "2021-03-08 16:42:02 WARN  Utils:66 - Your hostname, intern01 resolves to a loopback address: 127.0.1.1; using 10.239.44.107 instead (on interface eno1)\n",
      "2021-03-08 16:42:02 WARN  Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "2021-03-08 16:42:02 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/home/jinglei/analytics-zoo/zoo/target/analytics-zoo-bigdl_0.12.1-spark_2.4.3-0.10.0-SNAPSHOT-jar-with-dependencies.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/intern/spark/spark-2.4.3-bin-hadoop2.7/jars/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "cls.getname: com.intel.analytics.bigdl.python.api.Sample\n",
      "BigDLBasePickler registering: bigdl.util.common  Sample\n",
      "cls.getname: com.intel.analytics.bigdl.python.api.EvaluatedResult\n",
      "BigDLBasePickler registering: bigdl.util.common  EvaluatedResult\n",
      "cls.getname: com.intel.analytics.bigdl.python.api.JTensor\n",
      "BigDLBasePickler registering: bigdl.util.common  JTensor\n",
      "cls.getname: com.intel.analytics.bigdl.python.api.JActivity\n",
      "BigDLBasePickler registering: bigdl.util.common  JActivitySuccessfully got a SparkContext\n",
      "\n",
      "\n",
      "User settings:\n",
      "\n",
      "   KMP_AFFINITY=granularity=fine,compact,1,0\n",
      "   KMP_BLOCKTIME=0\n",
      "   KMP_SETTINGS=1\n",
      "   OMP_NUM_THREADS=1\n",
      "\n",
      "Effective settings:\n",
      "\n",
      "   KMP_ABORT_DELAY=0\n",
      "   KMP_ADAPTIVE_LOCK_PROPS='1,1024'\n",
      "   KMP_ALIGN_ALLOC=64\n",
      "   KMP_ALL_THREADPRIVATE=128\n",
      "   KMP_ATOMIC_MODE=2\n",
      "   KMP_BLOCKTIME=0\n",
      "   KMP_CPUINFO_FILE: value is not defined\n",
      "   KMP_DETERMINISTIC_REDUCTION=false\n",
      "   KMP_DEVICE_THREAD_LIMIT=2147483647\n",
      "   KMP_DISP_HAND_THREAD=false\n",
      "   KMP_DISP_NUM_BUFFERS=7\n",
      "   KMP_DUPLICATE_LIB_OK=false\n",
      "   KMP_FORCE_REDUCTION: value is not defined\n",
      "   KMP_FOREIGN_THREADS_THREADPRIVATE=true\n",
      "   KMP_FORKJOIN_BARRIER='2,2'\n",
      "   KMP_FORKJOIN_BARRIER_PATTERN='hyper,hyper'\n",
      "   KMP_FORKJOIN_FRAMES=true\n",
      "   KMP_FORKJOIN_FRAMES_MODE=3\n",
      "   KMP_GTID_MODE=3\n",
      "   KMP_HANDLE_SIGNALS=false\n",
      "   KMP_HOT_TEAMS_MAX_LEVEL=1\n",
      "   KMP_HOT_TEAMS_MODE=0\n",
      "   KMP_INIT_AT_FORK=true\n",
      "   KMP_INIT_WAIT=2048\n",
      "   KMP_ITT_PREPARE_DELAY=0\n",
      "   KMP_LIBRARY=throughput\n",
      "   KMP_LOCK_KIND=queuing\n",
      "   KMP_MALLOC_POOL_INCR=1M\n",
      "   KMP_NEXT_WAIT=1024\n",
      "   KMP_NUM_LOCKS_IN_BLOCK=1\n",
      "   KMP_PLAIN_BARRIER='2,2'\n",
      "   KMP_PLAIN_BARRIER_PATTERN='hyper,hyper'\n",
      "   KMP_REDUCTION_BARRIER='1,1'\n",
      "   KMP_REDUCTION_BARRIER_PATTERN='hyper,hyper'\n",
      "   KMP_SCHEDULE='static,balanced;guided,iterative'\n",
      "   KMP_SETTINGS=true\n",
      "   KMP_SPIN_BACKOFF_PARAMS='4096,100'\n",
      "   KMP_STACKOFFSET=64\n",
      "   KMP_STACKPAD=0\n",
      "   KMP_STACKSIZE=4M\n",
      "   KMP_STORAGE_MAP=false\n",
      "   KMP_TASKING=2\n",
      "   KMP_TASKLOOP_MIN_TASKS=0\n",
      "   KMP_TASK_STEALING_CONSTRAINT=1\n",
      "   KMP_TEAMS_THREAD_LIMIT=8\n",
      "   KMP_TOPOLOGY_METHOD=all\n",
      "   KMP_USER_LEVEL_MWAIT=false\n",
      "   KMP_VERSION=false\n",
      "   KMP_WARNINGS=true\n",
      "   OMP_AFFINITY_FORMAT='OMP: pid %P tid %T thread %n bound to OS proc set {%a}'\n",
      "   OMP_ALLOCATOR=omp_default_mem_alloc\n",
      "   OMP_CANCELLATION=false\n",
      "   OMP_DEFAULT_DEVICE=0\n",
      "   OMP_DISPLAY_AFFINITY=false\n",
      "   OMP_DISPLAY_ENV=false\n",
      "   OMP_DYNAMIC=false\n",
      "   OMP_MAX_ACTIVE_LEVELS=2147483647\n",
      "   OMP_MAX_TASK_PRIORITY=0\n",
      "   OMP_NESTED=false\n",
      "   OMP_NUM_THREADS='1'\n",
      "   OMP_PLACES: value is not defined\n",
      "   OMP_PROC_BIND='intel'\n",
      "   OMP_SCHEDULE='static'\n",
      "   OMP_STACKSIZE=4M\n",
      "   OMP_TARGET_OFFLOAD=DEFAULT\n",
      "   OMP_THREAD_LIMIT=2147483647\n",
      "   OMP_TOOL=enabled\n",
      "   OMP_TOOL_LIBRARIES: value is not defined\n",
      "   OMP_WAIT_POLICY=PASSIVE\n",
      "   KMP_AFFINITY='noverbose,warnings,respect,granularity=fine,compact,1,0'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from zoo.orca import init_orca_context, stop_orca_context\n",
    "from zoo.orca import OrcaContext\n",
    "\n",
    "# recommended to set it to True when running Analytics Zoo in Jupyter notebook. \n",
    "OrcaContext.log_output = True # (this will display terminal's stdout and stderr in the Jupyter notebook).\n",
    "\n",
    "cluster_mode = \"local\"\n",
    "\n",
    "if cluster_mode == \"local\":\n",
    "    init_orca_context(cores=1, memory=\"2g\")   # run in local mode\n",
    "elif cluster_mode == \"k8s\":\n",
    "    init_orca_context(cluster_mode=\"k8s\", num_nodes=2, cores=4) # run on K8s cluster\n",
    "elif cluster_mode == \"yarn\":\n",
    "    init_orca_context(\n",
    "        cluster_mode=\"yarn-client\", cores=4, num_nodes=2, memory=\"2g\",\n",
    "        driver_memory=\"10g\", driver_cores=1,\n",
    "        conf={\"spark.rpc.message.maxSize\": \"1024\",\n",
    "              \"spark.task.maxFailures\": \"1\",\n",
    "              \"spark.driver.extraJavaOptions\": \"-Dbigdl.failure.retryTimes=1\"})   # run on Hadoop YARN cluster"
   ]
  },
  {
   "source": [
    "Create a simple recurrent network using an Embedding layer and an RNN layer:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "SimpleRNN(\n  (embedding): Embedding(10000, 32)\n  (rnn): RNN(32, 32)\n  (fc): Linear(in_features=32, out_features=1, bias=True)\n  (out_act): Sigmoid()\n)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, 32)\n",
    "        for name, param in self.embedding.named_parameters(): \n",
    "            torch.nn.init.uniform_(param)\n",
    "        \n",
    "        self.rnn = nn.RNN(32, 32)\n",
    "        for name, param in self.rnn.named_parameters(): \n",
    "            if 'weight_ih' in name:\n",
    "                torch.nn.init.xavier_uniform_(param)\n",
    "            elif 'weight_hh' in name:\n",
    "                torch.nn.init.orthogonal_(param)\n",
    "            elif 'bias' in name:\n",
    "                torch.nn.init.zeros_(param)\n",
    "                \n",
    "        self.fc = nn.Linear(32, 1)\n",
    "        for name, param in self.rnn.named_parameters(): \n",
    "            if 'weight_ih' in name:\n",
    "                torch.nn.init.xavier_uniform_(param)\n",
    "            elif 'bias' in name:\n",
    "                torch.nn.init.zeros_(param)\n",
    "                \n",
    "        self.out_act = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text.transpose(0, 1))     # text: [128, 500] embedded: [500, 128, 32]\n",
    "        output, hidden = self.rnn(embedded)                 # hidden: [1, 128, 32]\n",
    "\n",
    "        es = \"output: \", output[-1,:,:].shape, \" hidden: \", hidden.shape\n",
    "        assert torch.equal(output[-1,:,:], hidden.squeeze(0)), es\n",
    "        \n",
    "        ret = self.fc(hidden.squeeze(0))                    # [128, 1]\n",
    "        #return ret.squeeze(1)\n",
    "        return self.out_act(ret)\n",
    "\n",
    "INPUT_DIM = 10000\n",
    "EMBEDDING_DIM = 32\n",
    "HIDDEN_DIM = 32\n",
    "\n",
    "model = SimpleRNN(INPUT_DIM)\n",
    "model.train()\n",
    "print(model)"
   ]
  },
  {
   "source": [
    "Specify loss function, optimizer and metrics:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zoo.orca.learn.metrics import Accuracy\n",
    "\n",
    "rmsprop = torch.optim.RMSprop(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.BCELoss()\n",
    "metrics=[Accuracy()]"
   ]
  },
  {
   "source": [
    "Now we can start the training:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "08 16:43:43 INFO  DistriOptimizer$:427 - [Epoch 1 12544/20096][Iteration 98][Wall Clock 32.400812604s] Trained 128.0 records in 0.377055273 seconds. Throughput is 339.47275 records/second. Loss is 0.6706658. \n",
      "2021-03-08 16:43:43 INFO  DistriOptimizer$:427 - [Epoch 1 12672/20096][Iteration 99][Wall Clock 32.787533765s] Trained 128.0 records in 0.386721161 seconds. Throughput is 330.98782 records/second. Loss is 0.6910979. \n",
      "2021-03-08 16:43:43 INFO  DistriOptimizer$:427 - [Epoch 1 12800/20096][Iteration 100][Wall Clock 33.18153258s] Trained 128.0 records in 0.393998815 seconds. Throughput is 324.87408 records/second. Loss is 0.6752796. \n",
      "2021-03-08 16:43:44 INFO  DistriOptimizer$:427 - [Epoch 1 12928/20096][Iteration 101][Wall Clock 33.564396072s] Trained 128.0 records in 0.382863492 seconds. Throughput is 334.3228 records/second. Loss is 0.69846493. \n",
      "2021-03-08 16:43:44 INFO  DistriOptimizer$:427 - [Epoch 1 13056/20096][Iteration 102][Wall Clock 33.958327341s] Trained 128.0 records in 0.393931269 seconds. Throughput is 324.92978 records/second. Loss is 0.6805232. \n",
      "2021-03-08 16:43:45 INFO  DistriOptimizer$:427 - [Epoch 1 13184/20096][Iteration 103][Wall Clock 34.333162631s] Trained 128.0 records in 0.37483529 seconds. Throughput is 341.48334 records/second. Loss is 0.6704349. \n",
      "2021-03-08 16:43:45 INFO  DistriOptimizer$:427 - [Epoch 1 13312/20096][Iteration 104][Wall Clock 34.728620368s] Trained 128.0 records in 0.395457737 seconds. Throughput is 323.67554 records/second. Loss is 0.6645647. \n",
      "2021-03-08 16:43:45 INFO  DistriOptimizer$:427 - [Epoch 1 13440/20096][Iteration 105][Wall Clock 35.116768657s] Trained 128.0 records in 0.388148289 seconds. Throughput is 329.77087 records/second. Loss is 0.71147984. \n",
      "2021-03-08 16:43:46 INFO  DistriOptimizer$:427 - [Epoch 1 13568/20096][Iteration 106][Wall Clock 35.510875762s] Trained 128.0 records in 0.394107105 seconds. Throughput is 324.78482 records/second. Loss is 0.6873729. \n",
      "2021-03-08 16:43:46 INFO  DistriOptimizer$:427 - [Epoch 1 13696/20096][Iteration 107][Wall Clock 35.903888078s] Trained 128.0 records in 0.393012316 seconds. Throughput is 325.68954 records/second. Loss is 0.6714569. \n",
      "2021-03-08 16:43:47 INFO  DistriOptimizer$:427 - [Epoch 1 13824/20096][Iteration 108][Wall Clock 36.290142009s] Trained 128.0 records in 0.386253931 seconds. Throughput is 331.3882 records/second. Loss is 0.66174114. \n",
      "2021-03-08 16:43:47 INFO  DistriOptimizer$:427 - [Epoch 1 13952/20096][Iteration 109][Wall Clock 36.686244326s] Trained 128.0 records in 0.396102317 seconds. Throughput is 323.14883 records/second. Loss is 0.66701424. \n",
      "2021-03-08 16:43:47 INFO  DistriOptimizer$:427 - [Epoch 1 14080/20096][Iteration 110][Wall Clock 37.09432847s] Trained 128.0 records in 0.408084144 seconds. Throughput is 313.6608 records/second. Loss is 0.6800538. \n",
      "2021-03-08 16:43:48 INFO  DistriOptimizer$:427 - [Epoch 1 14208/20096][Iteration 111][Wall Clock 37.509448099s] Trained 128.0 records in 0.415119629 seconds. Throughput is 308.34485 records/second. Loss is 0.69431734. \n",
      "2021-03-08 16:43:48 INFO  DistriOptimizer$:427 - [Epoch 1 14336/20096][Iteration 112][Wall Clock 37.918138162s] Trained 128.0 records in 0.408690063 seconds. Throughput is 313.19577 records/second. Loss is 0.71392626. \n",
      "2021-03-08 16:43:49 INFO  DistriOptimizer$:427 - [Epoch 1 14464/20096][Iteration 113][Wall Clock 38.336902808s] Trained 128.0 records in 0.418764646 seconds. Throughput is 305.66095 records/second. Loss is 0.67002314. \n",
      "2021-03-08 16:43:49 INFO  DistriOptimizer$:427 - [Epoch 1 14592/20096][Iteration 114][Wall Clock 38.751064732s] Trained 128.0 records in 0.414161924 seconds. Throughput is 309.05786 records/second. Loss is 0.6898402. \n",
      "2021-03-08 16:43:49 INFO  DistriOptimizer$:427 - [Epoch 1 14720/20096][Iteration 115][Wall Clock 39.15434934s] Trained 128.0 records in 0.403284608 seconds. Throughput is 317.3937 records/second. Loss is 0.67081434. \n",
      "2021-03-08 16:43:50 INFO  DistriOptimizer$:427 - [Epoch 1 14848/20096][Iteration 116][Wall Clock 39.57154674s] Trained 128.0 records in 0.4171974 seconds. Throughput is 306.8092 records/second. Loss is 0.6655725. \n",
      "2021-03-08 16:43:50 INFO  DistriOptimizer$:427 - [Epoch 1 14976/20096][Iteration 117][Wall Clock 39.973043591s] Trained 128.0 records in 0.401496851 seconds. Throughput is 318.80698 records/second. Loss is 0.6738015. \n",
      "2021-03-08 16:43:51 INFO  DistriOptimizer$:427 - [Epoch 1 15104/20096][Iteration 118][Wall Clock 40.366913864s] Trained 128.0 records in 0.393870273 seconds. Throughput is 324.9801 records/second. Loss is 0.70110714. \n",
      "2021-03-08 16:43:51 INFO  DistriOptimizer$:427 - [Epoch 1 15232/20096][Iteration 119][Wall Clock 40.789998512s] Trained 128.0 records in 0.423084648 seconds. Throughput is 302.53992 records/second. Loss is 0.67738116. \n",
      "2021-03-08 16:43:51 INFO  DistriOptimizer$:427 - [Epoch 1 15360/20096][Iteration 120][Wall Clock 41.207586176s] Trained 128.0 records in 0.417587664 seconds. Throughput is 306.52246 records/second. Loss is 0.66939276. \n",
      "2021-03-08 16:43:52 INFO  DistriOptimizer$:427 - [Epoch 1 15488/20096][Iteration 121][Wall Clock 41.613662735s] Trained 128.0 records in 0.406076559 seconds. Throughput is 315.21152 records/second. Loss is 0.677701. \n",
      "2021-03-08 16:43:52 INFO  DistriOptimizer$:427 - [Epoch 1 15616/20096][Iteration 122][Wall Clock 42.017091711s] Trained 128.0 records in 0.403428976 seconds. Throughput is 317.28015 records/second. Loss is 0.6765777. \n",
      "2021-03-08 16:43:53 INFO  DistriOptimizer$:427 - [Epoch 1 15744/20096][Iteration 123][Wall Clock 42.427199534s] Trained 128.0 records in 0.410107823 seconds. Throughput is 312.11304 records/second. Loss is 0.672777. \n",
      "2021-03-08 16:43:53 INFO  DistriOptimizer$:427 - [Epoch 1 15872/20096][Iteration 124][Wall Clock 42.842059197s] Trained 128.0 records in 0.414859663 seconds. Throughput is 308.5381 records/second. Loss is 0.6787495. \n",
      "2021-03-08 16:43:53 INFO  DistriOptimizer$:427 - [Epoch 1 16000/20096][Iteration 125][Wall Clock 43.240063869s] Trained 128.0 records in 0.398004672 seconds. Throughput is 321.60425 records/second. Loss is 0.71399343. \n",
      "2021-03-08 16:43:54 INFO  DistriOptimizer$:427 - [Epoch 1 16128/20096][Iteration 126][Wall Clock 43.619391001s] Trained 128.0 records in 0.379327132 seconds. Throughput is 337.43964 records/second. Loss is 0.68687475. \n",
      "2021-03-08 16:43:54 INFO  DistriOptimizer$:427 - [Epoch 1 16256/20096][Iteration 127][Wall Clock 43.984691238s] Trained 128.0 records in 0.365300237 seconds. Throughput is 350.3967 records/second. Loss is 0.69203126. \n",
      "2021-03-08 16:43:55 INFO  DistriOptimizer$:427 - [Epoch 1 16384/20096][Iteration 128][Wall Clock 44.362919513s] Trained 128.0 records in 0.378228275 seconds. Throughput is 338.41995 records/second. Loss is 0.67796373. \n",
      "2021-03-08 16:43:55 INFO  DistriOptimizer$:427 - [Epoch 1 16512/20096][Iteration 129][Wall Clock 44.734701675s] Trained 128.0 records in 0.371782162 seconds. Throughput is 344.28763 records/second. Loss is 0.67693. \n",
      "2021-03-08 16:43:55 INFO  DistriOptimizer$:427 - [Epoch 1 16640/20096][Iteration 130][Wall Clock 45.112141449s] Trained 128.0 records in 0.377439774 seconds. Throughput is 339.12695 records/second. Loss is 0.66923034. \n",
      "2021-03-08 16:43:56 INFO  DistriOptimizer$:427 - [Epoch 1 16768/20096][Iteration 131][Wall Clock 45.487870246s] Trained 128.0 records in 0.375728797 seconds. Throughput is 340.67126 records/second. Loss is 0.6967261. \n",
      "2021-03-08 16:43:56 INFO  DistriOptimizer$:427 - [Epoch 1 16896/20096][Iteration 132][Wall Clock 45.893780652s] Trained 128.0 records in 0.405910406 seconds. Throughput is 315.3405 records/second. Loss is 0.66856885. \n",
      "2021-03-08 16:43:57 INFO  DistriOptimizer$:427 - [Epoch 1 17024/20096][Iteration 133][Wall Clock 46.287169284s] Trained 128.0 records in 0.393388632 seconds. Throughput is 325.378 records/second. Loss is 0.6822742. \n",
      "2021-03-08 16:43:57 INFO  DistriOptimizer$:427 - [Epoch 1 17152/20096][Iteration 134][Wall Clock 46.695990576s] Trained 128.0 records in 0.408821292 seconds. Throughput is 313.09525 records/second. Loss is 0.66663134. \n",
      "2021-03-08 16:43:57 INFO  DistriOptimizer$:427 - [Epoch 1 17280/20096][Iteration 135][Wall Clock 47.095731226s] Trained 128.0 records in 0.39974065 seconds. Throughput is 320.2076 records/second. Loss is 0.68163. \n",
      "2021-03-08 16:43:58 INFO  DistriOptimizer$:427 - [Epoch 1 17408/20096][Iteration 136][Wall Clock 47.45834061s] Trained 128.0 records in 0.362609384 seconds. Throughput is 352.99692 records/second. Loss is 0.69682556. \n",
      "2021-03-08 16:43:58 INFO  DistriOptimizer$:427 - [Epoch 1 17536/20096][Iteration 137][Wall Clock 47.864168867s] Trained 128.0 records in 0.405828257 seconds. Throughput is 315.40436 records/second. Loss is 0.6839136. \n",
      "2021-03-08 16:43:58 INFO  DistriOptimizer$:427 - [Epoch 1 17664/20096][Iteration 138][Wall Clock 48.234640783s] Trained 128.0 records in 0.370471916 seconds. Throughput is 345.50525 records/second. Loss is 0.68562293. \n",
      "2021-03-08 16:43:59 INFO  DistriOptimizer$:427 - [Epoch 1 17792/20096][Iteration 139][Wall Clock 48.597976537s] Trained 128.0 records in 0.363335754 seconds. Throughput is 352.29123 records/second. Loss is 0.666903. \n",
      "2021-03-08 16:43:59 INFO  DistriOptimizer$:427 - [Epoch 1 17920/20096][Iteration 140][Wall Clock 48.964287778s] Trained 128.0 records in 0.366311241 seconds. Throughput is 349.42963 records/second. Loss is 0.71023345. \n",
      "2021-03-08 16:44:00 INFO  DistriOptimizer$:427 - [Epoch 1 18048/20096][Iteration 141][Wall Clock 49.355095955s] Trained 128.0 records in 0.390808177 seconds. Throughput is 327.52643 records/second. Loss is 0.6740053. \n",
      "2021-03-08 16:44:00 INFO  DistriOptimizer$:427 - [Epoch 1 18176/20096][Iteration 142][Wall Clock 49.744468744s] Trained 128.0 records in 0.389372789 seconds. Throughput is 328.7338 records/second. Loss is 0.6773147. \n",
      "2021-03-08 16:44:00 INFO  DistriOptimizer$:427 - [Epoch 1 18304/20096][Iteration 143][Wall Clock 50.133037672s] Trained 128.0 records in 0.388568928 seconds. Throughput is 329.41388 records/second. Loss is 0.6976871. \n",
      "2021-03-08 16:44:01 INFO  DistriOptimizer$:427 - [Epoch 1 18432/20096][Iteration 144][Wall Clock 50.531314813s] Trained 128.0 records in 0.398277141 seconds. Throughput is 321.38425 records/second. Loss is 0.67087865. \n",
      "2021-03-08 16:44:01 INFO  DistriOptimizer$:427 - [Epoch 1 18560/20096][Iteration 145][Wall Clock 50.920004713s] Trained 128.0 records in 0.3886899 seconds. Throughput is 329.31137 records/second. Loss is 0.68595564. \n",
      "2021-03-08 16:44:02 INFO  DistriOptimizer$:427 - [Epoch 1 18688/20096][Iteration 146][Wall Clock 51.322721768s] Trained 128.0 records in 0.402717055 seconds. Throughput is 317.84103 records/second. Loss is 0.6458092. \n",
      "2021-03-08 16:44:02 INFO  DistriOptimizer$:427 - [Epoch 1 18816/20096][Iteration 147][Wall Clock 51.723083568s] Trained 128.0 records in 0.4003618 seconds. Throughput is 319.71082 records/second. Loss is 0.67869604. \n",
      "2021-03-08 16:44:02 INFO  DistriOptimizer$:427 - [Epoch 1 18944/20096][Iteration 148][Wall Clock 52.133320465s] Trained 128.0 records in 0.410236897 seconds. Throughput is 312.01483 records/second. Loss is 0.6964238. \n",
      "2021-03-08 16:44:03 INFO  DistriOptimizer$:427 - [Epoch 1 19072/20096][Iteration 149][Wall Clock 52.550030684s] Trained 128.0 records in 0.416710219 seconds. Throughput is 307.16788 records/second. Loss is 0.66340804. \n",
      "2021-03-08 16:44:03 INFO  DistriOptimizer$:427 - [Epoch 1 19200/20096][Iteration 150][Wall Clock 52.949556248s] Trained 128.0 records in 0.399525564 seconds. Throughput is 320.38 records/second. Loss is 0.6565504. \n",
      "2021-03-08 16:44:04 INFO  DistriOptimizer$:427 - [Epoch 1 19328/20096][Iteration 151][Wall Clock 53.360129362s] Trained 128.0 records in 0.410573114 seconds. Throughput is 311.75934 records/second. Loss is 0.67472064. \n",
      "2021-03-08 16:44:04 INFO  DistriOptimizer$:427 - [Epoch 1 19456/20096][Iteration 152][Wall Clock 53.762499796s] Trained 128.0 records in 0.402370434 seconds. Throughput is 318.11484 records/second. Loss is 0.6636511. \n",
      "2021-03-08 16:44:04 INFO  DistriOptimizer$:427 - [Epoch 1 19584/20096][Iteration 153][Wall Clock 54.165277994s] Trained 128.0 records in 0.402778198 seconds. Throughput is 317.79276 records/second. Loss is 0.67117554. \n",
      "2021-03-08 16:44:05 INFO  DistriOptimizer$:427 - [Epoch 1 19712/20096][Iteration 154][Wall Clock 54.597772929s] Trained 128.0 records in 0.432494935 seconds. Throughput is 295.9572 records/second. Loss is 0.68035626. \n",
      "2021-03-08 16:44:05 INFO  DistriOptimizer$:427 - [Epoch 1 19840/20096][Iteration 155][Wall Clock 55.016623203s] Trained 128.0 records in 0.418850274 seconds. Throughput is 305.59845 records/second. Loss is 0.6726905. \n",
      "2021-03-08 16:44:06 INFO  DistriOptimizer$:427 - [Epoch 1 19968/20096][Iteration 156][Wall Clock 55.425727243s] Trained 128.0 records in 0.40910404 seconds. Throughput is 312.87885 records/second. Loss is 0.68396366. \n",
      "2021-03-08 16:44:06 INFO  DistriOptimizer$:427 - [Epoch 1 20096/20096][Iteration 157][Wall Clock 55.572793658s] Trained 128.0 records in 0.147066415 seconds. Throughput is 870.3551 records/second. Loss is 0.6962302. \n",
      "2021-03-08 16:44:06 INFO  DistriOptimizer$:472 - [Epoch 1 20096/20096][Iteration 157][Wall Clock 55.572793658s] Epoch finished. Wall clock time is 55785.389107 ms\n",
      "2021-03-08 16:44:06 INFO  DistriOptimizer$:111 - [Epoch 1 20096/20096][Iteration 157][Wall Clock 55.572793658s] Validate model...\n",
      "[Stage 323:>                                                        (0 + 1) / 1]Warn: jep.JepException: <class 'StopIteration'>\n",
      "\tat jep.Jep.exec(Native Method)\n",
      "\tat jep.Jep.exec(Jep.java:478)\n",
      "\tat com.intel.analytics.zoo.common.PythonInterpreter$$anonfun$1.apply$mcV$sp(PythonInterpreter.scala:108)\n",
      "\tat com.intel.analytics.zoo.common.PythonInterpreter$$anonfun$1.apply(PythonInterpreter.scala:107)\n",
      "\tat com.intel.analytics.zoo.common.PythonInterpreter$$anonfun$1.apply(PythonInterpreter.scala:107)\n",
      "\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n",
      "\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2021-03-08 16:44:07 INFO  DistriOptimizer$:177 - [Epoch 1 20096/20096][Iteration 157][Wall Clock 55.572793658s] validate model throughput is 3329.6182 records/second\n",
      "2021-03-08 16:44:07 INFO  DistriOptimizer$:180 - [Epoch 1 20096/20096][Iteration 157][Wall Clock 55.572793658s] Top1Accuracy is Accuracy(correct: 2941, count: 5000, accuracy: 0.5882)\n",
      "\n",
      "fit  1 end\n",
      "\n",
      "Get training accuracy: \n",
      "[Stage 327:>                                                        (0 + 1) / 1]Warn: jep.JepException: <class 'StopIteration'>\n",
      "\tat jep.Jep.exec(Native Method)\n",
      "\tat jep.Jep.exec(Jep.java:478)\n",
      "\tat com.intel.analytics.zoo.common.PythonInterpreter$$anonfun$1.apply$mcV$sp(PythonInterpreter.scala:108)\n",
      "\tat com.intel.analytics.zoo.common.PythonInterpreter$$anonfun$1.apply(PythonInterpreter.scala:107)\n",
      "\tat com.intel.analytics.zoo.common.PythonInterpreter$$anonfun$1.apply(PythonInterpreter.scala:107)\n",
      "\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n",
      "\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2021-03-08 16:44:29 INFO  DistriOptimizer$:1759 - Top1Accuracy is Accuracy(correct: 12404, count: 20000, accuracy: 0.6202)\n",
      "\n",
      "fit  2 start\n",
      "\n",
      "creating: createEveryEpoch\n",
      "creating: createMaxEpoch\n",
      "2021-03-08 16:44:48 INFO  DistriOptimizer$:818 - caching training rdd ...\n",
      "2021-03-08 16:45:13 INFO  DistriOptimizer$:161 - Count dataset\n",
      "Warn: jep.JepException: <class 'StopIteration'>\n",
      "\tat jep.Jep.exec(Native Method)\n",
      "\tat jep.Jep.exec(Jep.java:478)\n",
      "\tat com.intel.analytics.zoo.common.PythonInterpreter$$anonfun$1.apply$mcV$sp(PythonInterpreter.scala:108)\n",
      "\tat com.intel.analytics.zoo.common.PythonInterpreter$$anonfun$1.apply(PythonInterpreter.scala:107)\n",
      "\tat com.intel.analytics.zoo.common.PythonInterpreter$$anonfun$1.apply(PythonInterpreter.scala:107)\n",
      "\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n",
      "\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2021-03-08 16:45:13 INFO  DistriOptimizer$:165 - Count dataset complete. Time elapsed: 0.186302138s\n",
      "Warn: jep.JepException: <class 'StopIteration'>\n",
      "\tat jep.Jep.exec(Native Method)\n",
      "\tat jep.Jep.exec(Jep.java:478)\n",
      "\tat com.intel.analytics.zoo.common.PythonInterpreter$$anonfun$1.apply$mcV$sp(PythonInterpreter.scala:108)\n",
      "\tat com.intel.analytics.zoo.common.PythonInterpreter$$anonfun$1.apply(PythonInterpreter.scala:107)\n",
      "\tat com.intel.analytics.zoo.common.PythonInterpreter$$anonfun$1.apply(PythonInterpreter.scala:107)\n",
      "\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n",
      "\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2021-03-08 16:45:13 WARN  DistriOptimizer$:167 - If the dataset is built directly from RDD[Minibatch], the data in each minibatch is fixed, and a single minibatch is randomly selected in each partition. If the dataset is transformed from RDD[Sample], each minibatch will be constructed on the fly from random samples, which is better for convergence.\n",
      "2021-03-08 16:45:13 INFO  DistriOptimizer$:173 - config  {\n",
      "\tcomputeThresholdbatchSize: 100\n",
      "\tmaxDropPercentage: 0.0\n",
      "\twarmupIterationNum: 200\n",
      "\tisLayerwiseScaled: false\n",
      "\tdropPercentage: 0.0\n",
      " }\n",
      "2021-03-08 16:45:13 INFO  DistriOptimizer$:177 - Shuffle data\n",
      "2021-03-08 16:45:13 INFO  DistriOptimizer$:180 - Shuffle data complete. Takes 1.56775E-4s\n",
      "\n",
      "fit  2 end\n",
      "\n",
      "Get training accuracy: \n",
      "[Stage 337:>                                                        (0 + 1) / 1]Warn: jep.JepException: <class 'StopIteration'>\n",
      "\tat jep.Jep.exec(Native Method)\n",
      "\tat jep.Jep.exec(Jep.java:478)\n",
      "\tat com.intel.analytics.zoo.common.PythonInterpreter$$anonfun$1.apply$mcV$sp(PythonInterpreter.scala:108)\n",
      "\tat com.intel.analytics.zoo.common.PythonInterpreter$$anonfun$1.apply(PythonInterpreter.scala:107)\n",
      "\tat com.intel.analytics.zoo.common.PythonInterpreter$$anonfun$1.apply(PythonInterpreter.scala:107)\n",
      "\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n",
      "\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2021-03-08 16:45:34 INFO  DistriOptimizer$:1759 - Top1Accuracy is Accuracy(correct: 12404, count: 20000, accuracy: 0.6202)\n",
      "[0.620199978351593, 0.620199978351593]\n",
      "[0.67833694252492, 0.67833694252492]\n"
     ]
    }
   ],
   "source": [
    "from zoo.orca.learn.pytorch import Estimator\n",
    "from zoo.orca.learn.trigger import EveryEpoch\n",
    "\n",
    "est = Estimator.from_torch(model=model, optimizer=rmsprop, loss=criterion, metrics=metrics)\n",
    "\n",
    "num_epochs = 2\n",
    "train_acc = []\n",
    "train_loss = []\n",
    "\n",
    "for i in range(1, num_epochs + 1):\n",
    "    est.set_tensorboard(\"./log/\", \"epoch_\" + str(i))\n",
    "    print(\"\\nfit \", i, \"start\\n\")\n",
    "    est.fit(data=train_loader, epochs=1, validation_data=val_loader, batch_size=batch_size, \n",
    "            checkpoint_trigger=EveryEpoch())\n",
    "    print(\"\\nfit \", i, \"end\\n\")\n",
    "\n",
    "    print(\"Get training accuracy: \")\n",
    "    train_acc_tmp = est.evaluate(data=train_loader, batch_size=batch_size)\n",
    "    train_acc.append(train_acc_tmp[\"Top1Accuracy\"])\n",
    "    train_loss_tmp = [_[1] for _ in est.get_train_summary(\"Loss\")]\n",
    "    train_loss.append(sum(train_loss_tmp) / len(train_loss_tmp))\n",
    "\n",
    "\n",
    "print(train_acc)\n",
    "print(train_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Stopping orca context\n"
     ]
    }
   ],
   "source": [
    "stop_orca_context()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('tf2': conda)",
   "metadata": {
    "interpreter": {
     "hash": "5056d60bd69d51d1e2df40a6c6e7c71a949f2ed4ad3fa2ff8fce432a7c7c0fe5"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}