{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'1.7.1'"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding recurrent neural networks\n",
    "\n",
    "This notebook contains code migrated from samples found in Chapter 6, Section 2 of [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff). Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.\n",
    "\n",
    "\n",
    "## A first recurrent layer in PyTorch\n",
    "\n",
    "The process we just naively implemented in Numpy corresponds to an actual PyTorch layer: the `RNN` layer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.nn import RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is just one minor difference: `RNN` processes batches of sequences, not just a single sequence like in our Numpy example. This means that it takes inputs of shape `(seq_len, batch, input_size)`, rather than `(seq_len, input_size)`.\n",
    "\n",
    "Like all recurrent layers in PyTorh, `RNN` returns the full sequences of successive outputs for each time stamp (a 3D tensor of shape `(seq_len, batch, num_directions * hidden_size)`), and a tensor `h_n` of shape `(num_layers * num_directions, batch, hidden_size)`, which contains the hidden state for `t = seq_len`.  Let's take a look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Network1(\n  (embedding): Embedding(10000, 32)\n  (rnn): RNN(32, 32)\n)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Network1(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim):\n",
    "        super(Network1, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        output, hidden = self.rnn(embedded)\n",
    "\n",
    "        return hidden   # return only the last output for each input sequence\n",
    "\n",
    "INPUT_DIM = 10000\n",
    "EMBEDDING_DIM = 32\n",
    "HIDDEN_DIM = 32\n",
    "\n",
    "model1 = Network1(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "print(model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is sometimes useful to stack several recurrent layers one after the other in order to increase the representational power of a network. \n",
    "In such a setup, you have to get all intermediate layers to return full sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Network2(\n  (embedding): Embedding(10000, 32)\n  (rnn1): RNN(32, 32)\n  (rnn2): RNN(32, 32)\n  (rnn3): RNN(32, 32)\n  (rnn4): RNN(32, 32)\n)\n"
     ]
    }
   ],
   "source": [
    "class Network2(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim):\n",
    "        super(Network2, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.rnn1 = nn.RNN(embedding_dim, hidden_dim)\n",
    "        self.rnn2 = nn.RNN(embedding_dim, hidden_dim)\n",
    "        self.rnn3 = nn.RNN(embedding_dim, hidden_dim)\n",
    "        self.rnn4 = nn.RNN(embedding_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        output, hidden = self.rnn1(embedded)\n",
    "        output, hidden = self.rnn2(output)\n",
    "        output, hidden = self.rnn3(output)\n",
    "        output, hidden = self.rnn4(output)\n",
    "\n",
    "        return hidden\n",
    "\n",
    "\n",
    "model2 = Network2(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "print(model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to use such a model on the IMDB movie review classification problem. We will try to build a machine learning model to classify movie reviews into 2 categories: postive or negtive. The IMDB training set contains 25000 movie reviews labeled by sentiment (positive / negative). Since our model cannot process strings directly, we will use a preprocessed dataset `imdb.npz`, which has turned the words in the reviews into integers according to the frequency they appear in the dataset. For instance, the integer \"3\" encodes the 3rd most frequent word in the data. \n",
    "\n",
    "Before loading the dataset, let's first define some parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_char = 1          # The start of a sequence will be marked with 1.\n",
    "num_words = 10000       # Number of words to consider as features. 10000 most frequent words are kept.\n",
    "maxlen = 500            # Maximum sequence length. Cut texts after this number of words.\n",
    "index_from=3            # Index actual words with this index and higher.\n",
    "oov_char=2              # Words that were cut out because of the num_words limit will be replaced with 2.\n",
    "pad_char = 0            # Padding value.\n",
    "batch_size = 128\n",
    "seed = 113"
   ]
  },
  {
   "source": [
    "Now we can begin our data processing. First let's load the dataset:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset loading finished.\nLength of training set:  25000\n"
     ]
    }
   ],
   "source": [
    "from bigdl.dataset import base\n",
    "import numpy as np\n",
    "\n",
    "def download_imdb(dest_dir):\n",
    "    \"\"\"Download pre-processed IMDB movie review data\n",
    "\n",
    "    :argument\n",
    "        dest_dir: destination directory to store the data\n",
    "    :return\n",
    "        The absolute path of the stored data\n",
    "    \"\"\"\n",
    "    file_name = \"imdb.npz\"\n",
    "    file_abs_path = base.maybe_download(file_name,\n",
    "                                        dest_dir,\n",
    "                                        'https://s3.amazonaws.com/text-datasets/imdb.npz')\n",
    "    return file_abs_path\n",
    "\n",
    "def load_imdb(dest_dir='/tmp/.bigdl/dataset'):\n",
    "    \"\"\"Load IMDB dataset.\n",
    "\n",
    "    :argument\n",
    "        dest_dir: where to cache the data (relative to `~/.bigdl/dataset`).\n",
    "    :return\n",
    "        the train, test separated IMDB dataset.\n",
    "    \"\"\"\n",
    "    path = download_imdb(dest_dir)\n",
    "    f = np.load(path, allow_pickle=True)\n",
    "    x_train = f['x_train']\n",
    "    y_train = f['y_train']\n",
    "    x_test = f['x_test']\n",
    "    y_test = f['y_test']\n",
    "    f.close()\n",
    "\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = load_imdb(dest_dir='.data/.bigdl/dataset')\n",
    "print(\"Dataset loading finished.\")\n",
    "print(\"Length of training set: \", len(x_train))"
   ]
  },
  {
   "source": [
    "Since only the training set of IMDB is used in this example, we will only preprocess the training set. Shuffle the data:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(seed)\n",
    "indices = np.arange(len(x_train))\n",
    "rng.shuffle(indices)\n",
    "x_train = x_train[indices]\n",
    "y_train = y_train[indices]"
   ]
  },
  {
   "source": [
    "Set the start character:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [[start_char] + [w + index_from for w in x] for x in x_train]"
   ]
  },
  {
   "source": [
    "Since we only consider `num_words` words (features) in this example, any word out of this range will be disgarded. The disgarded words are represented as `oov_char`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [[w if (w < num_words) else oov_char for w in x] for x in x_train]"
   ]
  },
  {
   "source": [
    "When we feed sequences into our model, all sequences in the batch need to be the same size: `maxlen`. Thus, to ensure each sentence in the batch is the same size, any shorter than `maxlen` needs to be padded, and any longer needs to be cut. `pad_char` is used to fill the blanks in the shorter sequences."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(x_train)):\n",
    "    l = len(x_train[i])\n",
    "    if l >= maxlen:\n",
    "        x_train[i] = x_train[i][(l - maxlen):]\n",
    "    else:\n",
    "        x_train[i] =[pad_char] * (maxlen - l) + x_train[i]"
   ]
  },
  {
   "source": [
    "To create a validation set, we can split the the training set with 25000 samples into two parts: 20000 samples for training and 5000 samples for validation."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature, train_label = x_train[0:20000], y_train[0:20000]\n",
    "val_feature, val_label = x_train[20000:25000], y_train[20000:25000]"
   ]
  },
  {
   "source": [
    "Create data loaders for the training and validation datasets:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def create_data_loader(feature, label, batch_size, shuffle=True):\n",
    "    feature_tensor = torch.LongTensor(feature)\n",
    "    label_tensor = torch.FloatTensor(label).unsqueeze(1)\n",
    "    dataset = TensorDataset(feature_tensor, label_tensor)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "train_loader = create_data_loader(train_feature, train_label, batch_size)\n",
    "val_loader = create_data_loader(val_feature, val_label, batch_size, False)"
   ]
  },
  {
   "source": [
    "Now we are ready to create and train the model. First, initialize orca context:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Initializing orca context\n",
      "Current pyspark location is : /intern/spark/spark-2.4.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/__init__.py\n",
      "Start to getOrCreate SparkContext\n",
      "pyspark_submit_args is:  --driver-class-path /home/jinglei/analytics-zoo/zoo/target/analytics-zoo-bigdl_0.12.1-spark_2.4.3-0.10.0-SNAPSHOT-jar-with-dependencies.jar pyspark-shell \n",
      "2021-03-08 18:02:59 WARN  Utils:66 - Your hostname, intern01 resolves to a loopback address: 127.0.1.1; using 10.239.44.107 instead (on interface eno1)\n",
      "2021-03-08 18:02:59 WARN  Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "2021-03-08 18:02:59 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/home/jinglei/analytics-zoo/zoo/target/analytics-zoo-bigdl_0.12.1-spark_2.4.3-0.10.0-SNAPSHOT-jar-with-dependencies.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/intern/spark/spark-2.4.3-bin-hadoop2.7/jars/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "cls.getname: com.intel.analytics.bigdl.python.api.Sample\n",
      "BigDLBasePickler registering: bigdl.util.common  Sample\n",
      "cls.getname: com.intel.analytics.bigdl.python.api.EvaluatedResult\n",
      "BigDLBasePickler registering: bigdl.util.common  EvaluatedResult\n",
      "cls.getname: com.intel.analytics.bigdl.python.api.JTensor\n",
      "BigDLBasePickler registering: bigdl.util.common  JTensor\n",
      "cls.getname: com.intel.analytics.bigdl.python.api.JActivity\n",
      "BigDLBasePickler registering: bigdl.util.common  JActivitySuccessfully got a SparkContext\n",
      "\n",
      "\n",
      "User settings:\n",
      "\n",
      "   KMP_AFFINITY=granularity=fine,compact,1,0\n",
      "   KMP_BLOCKTIME=0\n",
      "   KMP_SETTINGS=1\n",
      "   OMP_NUM_THREADS=1\n",
      "\n",
      "Effective settings:\n",
      "\n",
      "   KMP_ABORT_DELAY=0\n",
      "   KMP_ADAPTIVE_LOCK_PROPS='1,1024'\n",
      "   KMP_ALIGN_ALLOC=64\n",
      "   KMP_ALL_THREADPRIVATE=128\n",
      "   KMP_ATOMIC_MODE=2\n",
      "   KMP_BLOCKTIME=0\n",
      "   KMP_CPUINFO_FILE: value is not defined\n",
      "   KMP_DETERMINISTIC_REDUCTION=false\n",
      "   KMP_DEVICE_THREAD_LIMIT=2147483647\n",
      "   KMP_DISP_HAND_THREAD=false\n",
      "   KMP_DISP_NUM_BUFFERS=7\n",
      "   KMP_DUPLICATE_LIB_OK=false\n",
      "   KMP_FORCE_REDUCTION: value is not defined\n",
      "   KMP_FOREIGN_THREADS_THREADPRIVATE=true\n",
      "   KMP_FORKJOIN_BARRIER='2,2'\n",
      "   KMP_FORKJOIN_BARRIER_PATTERN='hyper,hyper'\n",
      "   KMP_FORKJOIN_FRAMES=true\n",
      "   KMP_FORKJOIN_FRAMES_MODE=3\n",
      "   KMP_GTID_MODE=3\n",
      "   KMP_HANDLE_SIGNALS=false\n",
      "   KMP_HOT_TEAMS_MAX_LEVEL=1\n",
      "   KMP_HOT_TEAMS_MODE=0\n",
      "   KMP_INIT_AT_FORK=true\n",
      "   KMP_INIT_WAIT=2048\n",
      "   KMP_ITT_PREPARE_DELAY=0\n",
      "   KMP_LIBRARY=throughput\n",
      "   KMP_LOCK_KIND=queuing\n",
      "   KMP_MALLOC_POOL_INCR=1M\n",
      "   KMP_NEXT_WAIT=1024\n",
      "   KMP_NUM_LOCKS_IN_BLOCK=1\n",
      "   KMP_PLAIN_BARRIER='2,2'\n",
      "   KMP_PLAIN_BARRIER_PATTERN='hyper,hyper'\n",
      "   KMP_REDUCTION_BARRIER='1,1'\n",
      "   KMP_REDUCTION_BARRIER_PATTERN='hyper,hyper'\n",
      "   KMP_SCHEDULE='static,balanced;guided,iterative'\n",
      "   KMP_SETTINGS=true\n",
      "   KMP_SPIN_BACKOFF_PARAMS='4096,100'\n",
      "   KMP_STACKOFFSET=64\n",
      "   KMP_STACKPAD=0\n",
      "   KMP_STACKSIZE=4M\n",
      "   KMP_STORAGE_MAP=false\n",
      "   KMP_TASKING=2\n",
      "   KMP_TASKLOOP_MIN_TASKS=0\n",
      "   KMP_TASK_STEALING_CONSTRAINT=1\n",
      "   KMP_TEAMS_THREAD_LIMIT=8\n",
      "   KMP_TOPOLOGY_METHOD=all\n",
      "   KMP_USER_LEVEL_MWAIT=false\n",
      "   KMP_VERSION=false\n",
      "   KMP_WARNINGS=true\n",
      "   OMP_AFFINITY_FORMAT='OMP: pid %P tid %T thread %n bound to OS proc set {%a}'\n",
      "   OMP_ALLOCATOR=omp_default_mem_alloc\n",
      "   OMP_CANCELLATION=false\n",
      "   OMP_DEFAULT_DEVICE=0\n",
      "   OMP_DISPLAY_AFFINITY=false\n",
      "   OMP_DISPLAY_ENV=false\n",
      "   OMP_DYNAMIC=false\n",
      "   OMP_MAX_ACTIVE_LEVELS=2147483647\n",
      "   OMP_MAX_TASK_PRIORITY=0\n",
      "   OMP_NESTED=false\n",
      "   OMP_NUM_THREADS='1'\n",
      "   OMP_PLACES: value is not defined\n",
      "   OMP_PROC_BIND='intel'\n",
      "   OMP_SCHEDULE='static'\n",
      "   OMP_STACKSIZE=4M\n",
      "   OMP_TARGET_OFFLOAD=DEFAULT\n",
      "   OMP_THREAD_LIMIT=2147483647\n",
      "   OMP_TOOL=enabled\n",
      "   OMP_TOOL_LIBRARIES: value is not defined\n",
      "   OMP_WAIT_POLICY=PASSIVE\n",
      "   KMP_AFFINITY='noverbose,warnings,respect,granularity=fine,compact,1,0'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from zoo.orca import init_orca_context, stop_orca_context\n",
    "from zoo.orca import OrcaContext\n",
    "\n",
    "# recommended to set it to True when running Analytics Zoo in Jupyter notebook. \n",
    "OrcaContext.log_output = True # (this will display terminal's stdout and stderr in the Jupyter notebook).\n",
    "\n",
    "cluster_mode = \"local\"\n",
    "\n",
    "if cluster_mode == \"local\":\n",
    "    init_orca_context(cores=1, memory=\"2g\")   # run in local mode\n",
    "elif cluster_mode == \"k8s\":\n",
    "    init_orca_context(cluster_mode=\"k8s\", num_nodes=2, cores=4) # run on K8s cluster\n",
    "elif cluster_mode == \"yarn\":\n",
    "    init_orca_context(\n",
    "        cluster_mode=\"yarn-client\", cores=4, num_nodes=2, memory=\"2g\",\n",
    "        driver_memory=\"10g\", driver_cores=1,\n",
    "        conf={\"spark.rpc.message.maxSize\": \"1024\",\n",
    "              \"spark.task.maxFailures\": \"1\",\n",
    "              \"spark.driver.extraJavaOptions\": \"-Dbigdl.failure.retryTimes=1\"})   # run on Hadoop YARN cluster"
   ]
  },
  {
   "source": [
    "Create a simple recurrent network using an Embedding layer and an RNN layer:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "SimpleRNN(\n  (embedding): Embedding(10000, 32)\n  (rnn): RNN(32, 32)\n  (fc): Linear(in_features=32, out_features=1, bias=True)\n  (out_act): Sigmoid()\n)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, 32)\n",
    "        for name, param in self.embedding.named_parameters(): \n",
    "            torch.nn.init.uniform_(param)\n",
    "        \n",
    "        self.rnn = nn.RNN(32, 32)\n",
    "        for name, param in self.rnn.named_parameters(): \n",
    "            if 'weight_ih' in name:\n",
    "                torch.nn.init.xavier_uniform_(param)\n",
    "            elif 'weight_hh' in name:\n",
    "                torch.nn.init.orthogonal_(param)\n",
    "            elif 'bias' in name:\n",
    "                torch.nn.init.zeros_(param)\n",
    "                \n",
    "        self.fc = nn.Linear(32, 1)\n",
    "        for name, param in self.rnn.named_parameters(): \n",
    "            if 'weight_ih' in name:\n",
    "                torch.nn.init.xavier_uniform_(param)\n",
    "            elif 'bias' in name:\n",
    "                torch.nn.init.zeros_(param)\n",
    "                \n",
    "        self.out_act = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text.transpose(0, 1))     # text: [128, 500] embedded: [500, 128, 32]\n",
    "        output, hidden = self.rnn(embedded)                 # hidden: [1, 128, 32]\n",
    "\n",
    "        es = \"output: \", output[-1,:,:].shape, \" hidden: \", hidden.shape\n",
    "        assert torch.equal(output[-1,:,:], hidden.squeeze(0)), es\n",
    "        \n",
    "        ret = self.fc(hidden.squeeze(0))                    # [128, 1]\n",
    "        #return ret.squeeze(1)\n",
    "        return self.out_act(ret)\n",
    "\n",
    "INPUT_DIM = 10000\n",
    "EMBEDDING_DIM = 32\n",
    "HIDDEN_DIM = 32\n",
    "\n",
    "model = SimpleRNN(INPUT_DIM)\n",
    "model.train()\n",
    "print(model)"
   ]
  },
  {
   "source": [
    "Specify loss function, optimizer and metrics:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zoo.orca.learn.metrics import Accuracy\n",
    "\n",
    "rmsprop = torch.optim.RMSprop(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.BCELoss()\n",
    "metrics=[Accuracy()]"
   ]
  },
  {
   "source": [
    "Now we can start the training:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ch 3 10112/20096][Iteration 393][Wall Clock 19.924019696s] Trained 128.0 records in 0.265249111 seconds. Throughput is 482.56525 records/second. Loss is 0.5812289. \n",
      "2021-03-08 18:08:14 INFO  DistriOptimizer$:427 - [Epoch 3 10240/20096][Iteration 394][Wall Clock 20.195453334s] Trained 128.0 records in 0.271433638 seconds. Throughput is 471.57013 records/second. Loss is 0.5284935. \n",
      "2021-03-08 18:08:14 INFO  DistriOptimizer$:427 - [Epoch 3 10368/20096][Iteration 395][Wall Clock 20.439586576s] Trained 128.0 records in 0.244133242 seconds. Throughput is 524.30383 records/second. Loss is 0.53677845. \n",
      "2021-03-08 18:08:15 INFO  DistriOptimizer$:427 - [Epoch 3 10496/20096][Iteration 396][Wall Clock 20.702462251s] Trained 128.0 records in 0.262875675 seconds. Throughput is 486.92218 records/second. Loss is 0.57641715. \n",
      "2021-03-08 18:08:15 INFO  DistriOptimizer$:427 - [Epoch 3 10624/20096][Iteration 397][Wall Clock 20.957317811s] Trained 128.0 records in 0.25485556 seconds. Throughput is 502.24524 records/second. Loss is 0.53488183. \n",
      "2021-03-08 18:08:15 INFO  DistriOptimizer$:427 - [Epoch 3 10752/20096][Iteration 398][Wall Clock 21.202520903s] Trained 128.0 records in 0.245203092 seconds. Throughput is 522.01624 records/second. Loss is 0.5226936. \n",
      "2021-03-08 18:08:15 INFO  DistriOptimizer$:427 - [Epoch 3 10880/20096][Iteration 399][Wall Clock 21.442599046s] Trained 128.0 records in 0.240078143 seconds. Throughput is 533.1597 records/second. Loss is 0.51059484. \n",
      "2021-03-08 18:08:16 INFO  DistriOptimizer$:427 - [Epoch 3 11008/20096][Iteration 400][Wall Clock 21.695371659s] Trained 128.0 records in 0.252772613 seconds. Throughput is 506.384 records/second. Loss is 0.50254893. \n",
      "2021-03-08 18:08:16 INFO  DistriOptimizer$:427 - [Epoch 3 11136/20096][Iteration 401][Wall Clock 21.937349169s] Trained 128.0 records in 0.24197751 seconds. Throughput is 528.9748 records/second. Loss is 0.4979973. \n",
      "2021-03-08 18:08:16 INFO  DistriOptimizer$:427 - [Epoch 3 11264/20096][Iteration 402][Wall Clock 22.206731927s] Trained 128.0 records in 0.269382758 seconds. Throughput is 475.16034 records/second. Loss is 0.54287666. \n",
      "2021-03-08 18:08:16 INFO  DistriOptimizer$:427 - [Epoch 3 11392/20096][Iteration 403][Wall Clock 22.472684918s] Trained 128.0 records in 0.265952991 seconds. Throughput is 481.28802 records/second. Loss is 0.6105561. \n",
      "2021-03-08 18:08:17 INFO  DistriOptimizer$:427 - [Epoch 3 11520/20096][Iteration 404][Wall Clock 22.719243258s] Trained 128.0 records in 0.24655834 seconds. Throughput is 519.1469 records/second. Loss is 0.54003793. \n",
      "2021-03-08 18:08:17 INFO  DistriOptimizer$:427 - [Epoch 3 11648/20096][Iteration 405][Wall Clock 22.956867608s] Trained 128.0 records in 0.23762435 seconds. Throughput is 538.66534 records/second. Loss is 0.5693815. \n",
      "2021-03-08 18:08:17 INFO  DistriOptimizer$:427 - [Epoch 3 11776/20096][Iteration 406][Wall Clock 23.204337585s] Trained 128.0 records in 0.247469977 seconds. Throughput is 517.23444 records/second. Loss is 0.5585442. \n",
      "2021-03-08 18:08:17 INFO  DistriOptimizer$:427 - [Epoch 3 11904/20096][Iteration 407][Wall Clock 23.468033125s] Trained 128.0 records in 0.26369554 seconds. Throughput is 485.4083 records/second. Loss is 0.5741186. \n",
      "2021-03-08 18:08:18 INFO  DistriOptimizer$:427 - [Epoch 3 12032/20096][Iteration 408][Wall Clock 23.716260585s] Trained 128.0 records in 0.24822746 seconds. Throughput is 515.65607 records/second. Loss is 0.52224493. \n",
      "2021-03-08 18:08:18 INFO  DistriOptimizer$:427 - [Epoch 3 12160/20096][Iteration 409][Wall Clock 23.969573404s] Trained 128.0 records in 0.253312819 seconds. Throughput is 505.30408 records/second. Loss is 0.55182666. \n",
      "2021-03-08 18:08:18 INFO  DistriOptimizer$:427 - [Epoch 3 12288/20096][Iteration 410][Wall Clock 24.212596061s] Trained 128.0 records in 0.243022657 seconds. Throughput is 526.6999 records/second. Loss is 0.47473872. \n",
      "2021-03-08 18:08:18 INFO  DistriOptimizer$:427 - [Epoch 3 12416/20096][Iteration 411][Wall Clock 24.451883534s] Trained 128.0 records in 0.239287473 seconds. Throughput is 534.92145 records/second. Loss is 0.47783107. \n",
      "2021-03-08 18:08:19 INFO  DistriOptimizer$:427 - [Epoch 3 12544/20096][Iteration 412][Wall Clock 24.708352024s] Trained 128.0 records in 0.25646849 seconds. Throughput is 499.0866 records/second. Loss is 0.52130306. \n",
      "2021-03-08 18:08:19 INFO  DistriOptimizer$:427 - [Epoch 3 12672/20096][Iteration 413][Wall Clock 24.952430181s] Trained 128.0 records in 0.244078157 seconds. Throughput is 524.4222 records/second. Loss is 0.48940495. \n",
      "2021-03-08 18:08:19 INFO  DistriOptimizer$:427 - [Epoch 3 12800/20096][Iteration 414][Wall Clock 25.190596615s] Trained 128.0 records in 0.238166434 seconds. Throughput is 537.4393 records/second. Loss is 0.4936351. \n",
      "2021-03-08 18:08:19 INFO  DistriOptimizer$:427 - [Epoch 3 12928/20096][Iteration 415][Wall Clock 25.453317165s] Trained 128.0 records in 0.26272055 seconds. Throughput is 487.2097 records/second. Loss is 0.6366836. \n",
      "2021-03-08 18:08:20 INFO  DistriOptimizer$:427 - [Epoch 3 13056/20096][Iteration 416][Wall Clock 25.711747106s] Trained 128.0 records in 0.258429941 seconds. Throughput is 495.29865 records/second. Loss is 0.8122867. \n",
      "2021-03-08 18:08:20 INFO  DistriOptimizer$:427 - [Epoch 3 13184/20096][Iteration 417][Wall Clock 25.972985413s] Trained 128.0 records in 0.261238307 seconds. Throughput is 489.9741 records/second. Loss is 0.55556726. \n",
      "2021-03-08 18:08:20 INFO  DistriOptimizer$:427 - [Epoch 3 13312/20096][Iteration 418][Wall Clock 26.217058689s] Trained 128.0 records in 0.244073276 seconds. Throughput is 524.4327 records/second. Loss is 0.49691424. \n",
      "2021-03-08 18:08:20 INFO  DistriOptimizer$:427 - [Epoch 3 13440/20096][Iteration 419][Wall Clock 26.468198263s] Trained 128.0 records in 0.251139574 seconds. Throughput is 509.67673 records/second. Loss is 0.6065153. \n",
      "2021-03-08 18:08:21 INFO  DistriOptimizer$:427 - [Epoch 3 13568/20096][Iteration 420][Wall Clock 26.711418343s] Trained 128.0 records in 0.24322008 seconds. Throughput is 526.27234 records/second. Loss is 0.46869463. \n",
      "2021-03-08 18:08:21 INFO  DistriOptimizer$:427 - [Epoch 3 13696/20096][Iteration 421][Wall Clock 26.956148732s] Trained 128.0 records in 0.244730389 seconds. Throughput is 523.02454 records/second. Loss is 0.6293534. \n",
      "2021-03-08 18:08:21 INFO  DistriOptimizer$:427 - [Epoch 3 13824/20096][Iteration 422][Wall Clock 27.218583405s] Trained 128.0 records in 0.262434673 seconds. Throughput is 487.74045 records/second. Loss is 0.54025716. \n",
      "2021-03-08 18:08:21 INFO  DistriOptimizer$:427 - [Epoch 3 13952/20096][Iteration 423][Wall Clock 27.460077248s] Trained 128.0 records in 0.241493843 seconds. Throughput is 530.03424 records/second. Loss is 0.5291186. \n",
      "2021-03-08 18:08:22 INFO  DistriOptimizer$:427 - [Epoch 3 14080/20096][Iteration 424][Wall Clock 27.73000131s] Trained 128.0 records in 0.269924062 seconds. Throughput is 474.20743 records/second. Loss is 0.4573922. \n",
      "2021-03-08 18:08:22 INFO  DistriOptimizer$:427 - [Epoch 3 14208/20096][Iteration 425][Wall Clock 27.978796801s] Trained 128.0 records in 0.248795491 seconds. Throughput is 514.47876 records/second. Loss is 0.5057912. \n",
      "2021-03-08 18:08:22 INFO  DistriOptimizer$:427 - [Epoch 3 14336/20096][Iteration 426][Wall Clock 28.242476625s] Trained 128.0 records in 0.263679824 seconds. Throughput is 485.4372 records/second. Loss is 0.7499243. \n",
      "2021-03-08 18:08:23 INFO  DistriOptimizer$:427 - [Epoch 3 14464/20096][Iteration 427][Wall Clock 28.494659799s] Trained 128.0 records in 0.252183174 seconds. Throughput is 507.56757 records/second. Loss is 0.5265144. \n",
      "2021-03-08 18:08:23 INFO  DistriOptimizer$:427 - [Epoch 3 14592/20096][Iteration 428][Wall Clock 28.747502249s] Trained 128.0 records in 0.25284245 seconds. Throughput is 506.24408 records/second. Loss is 0.49428073. \n",
      "2021-03-08 18:08:23 INFO  DistriOptimizer$:427 - [Epoch 3 14720/20096][Iteration 429][Wall Clock 28.987507509s] Trained 128.0 records in 0.24000526 seconds. Throughput is 533.32166 records/second. Loss is 0.52792054. \n",
      "2021-03-08 18:08:23 INFO  DistriOptimizer$:427 - [Epoch 3 14848/20096][Iteration 430][Wall Clock 29.237054307s] Trained 128.0 records in 0.249546798 seconds. Throughput is 512.9299 records/second. Loss is 0.4710347. \n",
      "2021-03-08 18:08:24 INFO  DistriOptimizer$:427 - [Epoch 3 14976/20096][Iteration 431][Wall Clock 29.487286125s] Trained 128.0 records in 0.250231818 seconds. Throughput is 511.52563 records/second. Loss is 0.465834. \n",
      "2021-03-08 18:08:24 INFO  DistriOptimizer$:427 - [Epoch 3 15104/20096][Iteration 432][Wall Clock 29.742021852s] Trained 128.0 records in 0.254735727 seconds. Throughput is 502.4815 records/second. Loss is 0.518712. \n",
      "2021-03-08 18:08:24 INFO  DistriOptimizer$:427 - [Epoch 3 15232/20096][Iteration 433][Wall Clock 29.985248491s] Trained 128.0 records in 0.243226639 seconds. Throughput is 526.2582 records/second. Loss is 0.5143454. \n",
      "2021-03-08 18:08:24 INFO  DistriOptimizer$:427 - [Epoch 3 15360/20096][Iteration 434][Wall Clock 30.224853828s] Trained 128.0 records in 0.239605337 seconds. Throughput is 534.2118 records/second. Loss is 0.46143648. \n",
      "2021-03-08 18:08:25 INFO  DistriOptimizer$:427 - [Epoch 3 15488/20096][Iteration 435][Wall Clock 30.475938113s] Trained 128.0 records in 0.251084285 seconds. Throughput is 509.78894 records/second. Loss is 0.5759045. \n",
      "2021-03-08 18:08:25 INFO  DistriOptimizer$:427 - [Epoch 3 15616/20096][Iteration 436][Wall Clock 30.724753289s] Trained 128.0 records in 0.248815176 seconds. Throughput is 514.43805 records/second. Loss is 0.47426608. \n",
      "2021-03-08 18:08:25 INFO  DistriOptimizer$:427 - [Epoch 3 15744/20096][Iteration 437][Wall Clock 30.971019701s] Trained 128.0 records in 0.246266412 seconds. Throughput is 519.7623 records/second. Loss is 0.47450727. \n",
      "2021-03-08 18:08:25 INFO  DistriOptimizer$:427 - [Epoch 3 15872/20096][Iteration 438][Wall Clock 31.239105896s] Trained 128.0 records in 0.268086195 seconds. Throughput is 477.45837 records/second. Loss is 0.5810969. \n",
      "2021-03-08 18:08:26 INFO  DistriOptimizer$:427 - [Epoch 3 16000/20096][Iteration 439][Wall Clock 31.478705416s] Trained 128.0 records in 0.23959952 seconds. Throughput is 534.2248 records/second. Loss is 0.6244718. \n",
      "2021-03-08 18:08:26 INFO  DistriOptimizer$:427 - [Epoch 3 16128/20096][Iteration 440][Wall Clock 31.715222476s] Trained 128.0 records in 0.23651706 seconds. Throughput is 541.1872 records/second. Loss is 0.5610592. \n",
      "2021-03-08 18:08:26 INFO  DistriOptimizer$:427 - [Epoch 3 16256/20096][Iteration 441][Wall Clock 31.958000992s] Trained 128.0 records in 0.242778516 seconds. Throughput is 527.22955 records/second. Loss is 0.55598706. \n",
      "2021-03-08 18:08:26 INFO  DistriOptimizer$:427 - [Epoch 3 16384/20096][Iteration 442][Wall Clock 32.218690619s] Trained 128.0 records in 0.260689627 seconds. Throughput is 491.00537 records/second. Loss is 0.5944627. \n",
      "2021-03-08 18:08:27 INFO  DistriOptimizer$:427 - [Epoch 3 16512/20096][Iteration 443][Wall Clock 32.471677581s] Trained 128.0 records in 0.252986962 seconds. Throughput is 505.95493 records/second. Loss is 0.5873496. \n",
      "2021-03-08 18:08:27 INFO  DistriOptimizer$:427 - [Epoch 3 16640/20096][Iteration 444][Wall Clock 32.719623083s] Trained 128.0 records in 0.247945502 seconds. Throughput is 516.2425 records/second. Loss is 0.5205699. \n",
      "2021-03-08 18:08:27 INFO  DistriOptimizer$:427 - [Epoch 3 16768/20096][Iteration 445][Wall Clock 32.956406738s] Trained 128.0 records in 0.236783655 seconds. Throughput is 540.5779 records/second. Loss is 0.46979392. \n",
      "2021-03-08 18:08:27 INFO  DistriOptimizer$:427 - [Epoch 3 16896/20096][Iteration 446][Wall Clock 33.203096475s] Trained 128.0 records in 0.246689737 seconds. Throughput is 518.87036 records/second. Loss is 0.5147383. \n",
      "2021-03-08 18:08:27 INFO  DistriOptimizer$:427 - [Epoch 3 17024/20096][Iteration 447][Wall Clock 33.440185035s] Trained 128.0 records in 0.23708856 seconds. Throughput is 539.8826 records/second. Loss is 0.6058566. \n",
      "2021-03-08 18:08:28 INFO  DistriOptimizer$:427 - [Epoch 3 17152/20096][Iteration 448][Wall Clock 33.705086264s] Trained 128.0 records in 0.264901229 seconds. Throughput is 483.19897 records/second. Loss is 0.5345639. \n",
      "2021-03-08 18:08:28 INFO  DistriOptimizer$:427 - [Epoch 3 17280/20096][Iteration 449][Wall Clock 33.954044788s] Trained 128.0 records in 0.248958524 seconds. Throughput is 514.14185 records/second. Loss is 0.5529158. \n",
      "2021-03-08 18:08:28 INFO  DistriOptimizer$:427 - [Epoch 3 17408/20096][Iteration 450][Wall Clock 34.21345661s] Trained 128.0 records in 0.259411822 seconds. Throughput is 493.42395 records/second. Loss is 0.7062979. \n",
      "2021-03-08 18:08:29 INFO  DistriOptimizer$:427 - [Epoch 3 17536/20096][Iteration 451][Wall Clock 34.470075556s] Trained 128.0 records in 0.256618946 seconds. Throughput is 498.79404 records/second. Loss is 0.5586153. \n",
      "2021-03-08 18:08:29 INFO  DistriOptimizer$:427 - [Epoch 3 17664/20096][Iteration 452][Wall Clock 34.712853111s] Trained 128.0 records in 0.242777555 seconds. Throughput is 527.2316 records/second. Loss is 0.48576084. \n",
      "2021-03-08 18:08:29 INFO  DistriOptimizer$:427 - [Epoch 3 17792/20096][Iteration 453][Wall Clock 34.956943316s] Trained 128.0 records in 0.244090205 seconds. Throughput is 524.3963 records/second. Loss is 0.50734264. \n",
      "2021-03-08 18:08:29 INFO  DistriOptimizer$:427 - [Epoch 3 17920/20096][Iteration 454][Wall Clock 35.201829797s] Trained 128.0 records in 0.244886481 seconds. Throughput is 522.69116 records/second. Loss is 0.5954521. \n",
      "2021-03-08 18:08:30 INFO  DistriOptimizer$:427 - [Epoch 3 18048/20096][Iteration 455][Wall Clock 35.466419842s] Trained 128.0 records in 0.264590045 seconds. Throughput is 483.76724 records/second. Loss is 0.59950644. \n",
      "2021-03-08 18:08:30 INFO  DistriOptimizer$:427 - [Epoch 3 18176/20096][Iteration 456][Wall Clock 35.711655706s] Trained 128.0 records in 0.245235864 seconds. Throughput is 521.94653 records/second. Loss is 0.4647838. \n",
      "2021-03-08 18:08:30 INFO  DistriOptimizer$:427 - [Epoch 3 18304/20096][Iteration 457][Wall Clock 35.949606633s] Trained 128.0 records in 0.237950927 seconds. Throughput is 537.926 records/second. Loss is 0.48468262. \n",
      "2021-03-08 18:08:30 INFO  DistriOptimizer$:427 - [Epoch 3 18432/20096][Iteration 458][Wall Clock 36.191357726s] Trained 128.0 records in 0.241751093 seconds. Throughput is 529.4702 records/second. Loss is 0.5143323. \n",
      "2021-03-08 18:08:30 INFO  DistriOptimizer$:427 - [Epoch 3 18560/20096][Iteration 459][Wall Clock 36.454044668s] Trained 128.0 records in 0.262686942 seconds. Throughput is 487.27203 records/second. Loss is 0.5131767. \n",
      "2021-03-08 18:08:31 INFO  DistriOptimizer$:427 - [Epoch 3 18688/20096][Iteration 460][Wall Clock 36.714843852s] Trained 128.0 records in 0.260799184 seconds. Throughput is 490.7991 records/second. Loss is 0.518812. \n",
      "2021-03-08 18:08:31 INFO  DistriOptimizer$:427 - [Epoch 3 18816/20096][Iteration 461][Wall Clock 36.956323661s] Trained 128.0 records in 0.241479809 seconds. Throughput is 530.065 records/second. Loss is 0.5113771. \n",
      "2021-03-08 18:08:31 INFO  DistriOptimizer$:427 - [Epoch 3 18944/20096][Iteration 462][Wall Clock 37.203195778s] Trained 128.0 records in 0.246872117 seconds. Throughput is 518.48706 records/second. Loss is 0.48550868. \n",
      "2021-03-08 18:08:31 INFO  DistriOptimizer$:427 - [Epoch 3 19072/20096][Iteration 463][Wall Clock 37.458811431s] Trained 128.0 records in 0.255615653 seconds. Throughput is 500.7518 records/second. Loss is 0.47110024. \n",
      "2021-03-08 18:08:32 INFO  DistriOptimizer$:427 - [Epoch 3 19200/20096][Iteration 464][Wall Clock 37.698478885s] Trained 128.0 records in 0.239667454 seconds. Throughput is 534.07336 records/second. Loss is 0.5519315. \n",
      "2021-03-08 18:08:32 INFO  DistriOptimizer$:427 - [Epoch 3 19328/20096][Iteration 465][Wall Clock 37.957447748s] Trained 128.0 records in 0.258968863 seconds. Throughput is 494.2679 records/second. Loss is 0.52624035. \n",
      "2021-03-08 18:08:32 INFO  DistriOptimizer$:427 - [Epoch 3 19456/20096][Iteration 466][Wall Clock 38.200103391s] Trained 128.0 records in 0.242655643 seconds. Throughput is 527.49646 records/second. Loss is 0.46414685. \n",
      "2021-03-08 18:08:32 INFO  DistriOptimizer$:427 - [Epoch 3 19584/20096][Iteration 467][Wall Clock 38.448293256s] Trained 128.0 records in 0.248189865 seconds. Throughput is 515.7342 records/second. Loss is 0.49689543. \n",
      "2021-03-08 18:08:33 INFO  DistriOptimizer$:427 - [Epoch 3 19712/20096][Iteration 468][Wall Clock 38.696926063s] Trained 128.0 records in 0.248632807 seconds. Throughput is 514.8154 records/second. Loss is 0.4310194. \n",
      "2021-03-08 18:08:33 INFO  DistriOptimizer$:427 - [Epoch 3 19840/20096][Iteration 469][Wall Clock 38.950139789s] Trained 128.0 records in 0.253213726 seconds. Throughput is 505.5018 records/second. Loss is 0.5294108. \n",
      "2021-03-08 18:08:33 INFO  DistriOptimizer$:427 - [Epoch 3 19968/20096][Iteration 470][Wall Clock 39.227978662s] Trained 128.0 records in 0.277838873 seconds. Throughput is 460.69864 records/second. Loss is 0.7316571. \n",
      "2021-03-08 18:08:33 INFO  DistriOptimizer$:427 - [Epoch 3 20096/20096][Iteration 471][Wall Clock 39.328609817s] Trained 128.0 records in 0.100631155 seconds. Throughput is 1271.9719 records/second. Loss is 0.5012663. \n",
      "2021-03-08 18:08:33 INFO  DistriOptimizer$:472 - [Epoch 3 20096/20096][Iteration 471][Wall Clock 39.328609817s] Epoch finished. Wall clock time is 39403.104351 ms\n",
      "2021-03-08 18:08:33 INFO  DistriOptimizer$:111 - [Epoch 3 20096/20096][Iteration 471][Wall Clock 39.328609817s] Validate model...\n",
      "[Stage 973:>                                                        (0 + 1) / 1]Warn: jep.JepException: <class 'StopIteration'>\n",
      "\tat jep.Jep.exec(Native Method)\n",
      "\tat jep.Jep.exec(Jep.java:478)\n",
      "\tat com.intel.analytics.zoo.common.PythonInterpreter$$anonfun$1.apply$mcV$sp(PythonInterpreter.scala:108)\n",
      "\tat com.intel.analytics.zoo.common.PythonInterpreter$$anonfun$1.apply(PythonInterpreter.scala:107)\n",
      "\tat com.intel.analytics.zoo.common.PythonInterpreter$$anonfun$1.apply(PythonInterpreter.scala:107)\n",
      "\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n",
      "\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2021-03-08 18:08:35 INFO  DistriOptimizer$:177 - [Epoch 3 20096/20096][Iteration 471][Wall Clock 39.328609817s] validate model throughput is 3537.8533 records/second\n",
      "2021-03-08 18:08:35 INFO  DistriOptimizer$:180 - [Epoch 3 20096/20096][Iteration 471][Wall Clock 39.328609817s] Top1Accuracy is Accuracy(correct: 3575, count: 5000, accuracy: 0.715)\n",
      "\n",
      "fit  3 end\n",
      "\n",
      "Get training accuracy: \n",
      "[Stage 977:>                                                        (0 + 1) / 1]Warn: jep.JepException: <class 'StopIteration'>\n",
      "\tat jep.Jep.exec(Native Method)\n",
      "\tat jep.Jep.exec(Jep.java:478)\n",
      "\tat com.intel.analytics.zoo.common.PythonInterpreter$$anonfun$1.apply$mcV$sp(PythonInterpreter.scala:108)\n",
      "\tat com.intel.analytics.zoo.common.PythonInterpreter$$anonfun$1.apply(PythonInterpreter.scala:107)\n",
      "\tat com.intel.analytics.zoo.common.PythonInterpreter$$anonfun$1.apply(PythonInterpreter.scala:107)\n",
      "\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n",
      "\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2021-03-08 18:08:56 INFO  DistriOptimizer$:1759 - Top1Accuracy is Accuracy(correct: 15629, count: 20000, accuracy: 0.78145)\n",
      "Get validation accuracy: \n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nprint(\"Training accuracy: \")\\nprint(train_acc)\\nprint(\"Training loss: \")\\nprint(train_loss)\\nprint(\"Validation accuracy: \")\\nprint(val_acc)\\nprint(\"Validation loss: \")\\nprint(val_loss)\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "from zoo.orca.learn.pytorch import Estimator\n",
    "from zoo.orca.learn.trigger import EveryEpoch\n",
    "\n",
    "est = Estimator.from_torch(model=model, optimizer=rmsprop, loss=criterion, metrics=metrics)\n",
    "\n",
    "num_epochs = 3\n",
    "train_acc = []\n",
    "train_loss = []\n",
    "val_acc = []\n",
    "val_loss = []\n",
    "for i in range(1, num_epochs + 1):\n",
    "    est.set_tensorboard(\"./log/\", \"epoch_\" + str(i))\n",
    "    print(\"\\nfit \", i, \"start\\n\")\n",
    "    est.fit(data=train_loader, epochs=i, validation_data=val_loader, batch_size=batch_size, \n",
    "            checkpoint_trigger=EveryEpoch())\n",
    "    print(\"\\nfit \", i, \"end\\n\")\n",
    "\n",
    "    print(\"Get training accuracy: \")\n",
    "    train_acc_tmp = est.evaluate(data=train_loader, batch_size=batch_size)\n",
    "    train_acc.append(train_acc_tmp[\"Top1Accuracy\"])\n",
    "    train_loss_tmp = est.get_train_summary(\"Loss\")\n",
    "    train_loss.append(train_loss_tmp)\n",
    "\n",
    "    print(\"Get validation accuracy: \")\n",
    "    val_acc_tmp = est.get_validation_summary(\"Top1Accuracy\")\n",
    "    val_acc.append(val_acc_tmp)\n",
    "    val_loss_tmp = est.get_validation_summary(\"Loss\")\n",
    "    val_loss.append(val_loss_tmp)\n",
    "\n",
    "'''\n",
    "print(\"Training accuracy: \")\n",
    "print(train_acc)\n",
    "print(\"Training loss: \")\n",
    "print(train_loss)\n",
    "print(\"Validation accuracy: \")\n",
    "print(val_acc)\n",
    "print(\"Validation loss: \")\n",
    "print(val_loss)\n",
    "'''\n",
    "# The output is too long, so I do not show it here for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Stopping orca context\n"
     ]
    }
   ],
   "source": [
    "stop_orca_context()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('tf2': conda)",
   "metadata": {
    "interpreter": {
     "hash": "5056d60bd69d51d1e2df40a6c6e7c71a949f2ed4ad3fa2ff8fce432a7c7c0fe5"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}