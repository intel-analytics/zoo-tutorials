{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'1.7.1'"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A first look at a neural network\n",
    "\n",
    "This notebook contains the code samples found in Chapter 2, Section 1 of [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff). Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.\n",
    "\n",
    "----\n",
    "\n",
    "We will now take a look at a first concrete example of a neural network, which makes use of the Python library PyTorch to learn to classify \n",
    "hand-written digits. Unless you already have experience with PyTorch or similar libraries, you will not understand everything about this \n",
    "first example right away. You probably haven't even installed PyTorch yet. Don't worry, that is perfectly fine. In the next chapter, we will \n",
    "review each element in our example and explain them in detail. So don't worry if some steps seem arbitrary or look like magic to you! \n",
    "We've got to start somewhere.\n",
    "\n",
    "The problem we are trying to solve here is to classify grayscale images of handwritten digits (28 pixels by 28 pixels), into their 10 \n",
    "categories (0 to 9). The dataset we will use is the MNIST dataset, a classic dataset in the machine learning community, which has been \n",
    "around for almost as long as the field itself and has been very intensively studied. It's a set of 60,000 training images, plus 10,000 test \n",
    "images, assembled by the National Institute of Standards and Technology (the NIST in MNIST) in the 1980s. You can think of \"solving\" MNIST \n",
    "as the \"Hello World\" of deep learning -- it's what you do to verify that your algorithms are working as expected. As you become a machine \n",
    "learning practitioner, you will see MNIST come up over and over again, in scientific papers, blog posts, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST dataset comes pre-loaded in PyTorch, which can be extracted as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "\n",
    "dir='./dataset'\n",
    "train_data = datasets.MNIST(dir, train=True, download=True)\n",
    "test_data = datasets.MNIST(dir, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `train_data` is the \"training set\" that the model will learn from. The model will then be tested on the \n",
    "\"test set\", `test_data`. Both `train_data` and `test_data` are composed of a set of sample images (`data`) and their corresponding labels (`train_labels` / `test_labels`), which is an array of digits ranging from 0 to 9. There is a one-to-one correspondence between the images and the labels.\n",
    "\n",
    "Let's have a look at the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "train_data.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "len(train_data.train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([5, 0, 4,  ..., 5, 6, 8])"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "train_data.train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([10000, 28, 28])"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "test_data.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "len(test_data.test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([7, 2, 1,  ..., 4, 5, 6])"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "test_data.test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our workflow will be as follow: first we will present our neural network with the training data, `train_data`. The \n",
    "network will then learn to associate images and labels. Finally, we will ask the network to produce predictions for `test_data`, and we will verify if these predictions match the labels from `test_data`.\n",
    "\n",
    "Let's build our network -- again, remember that you aren't supposed to understand everything about this example just yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Network(\n",
       "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Construct model\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(28*28, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "model = Network()\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The core building block of neural networks is the \"layer\", a data-processing module which you can conceive as a \"filter\" for data. Some \n",
    "data comes in, and comes out in a more useful form. Precisely, layers extract _representations_ out of the data fed into them -- hopefully \n",
    "representations that are more meaningful for the problem at hand. Most of deep learning really consists of chaining together simple layers \n",
    "which will implement a form of progressive \"data distillation\". A deep learning model is like a sieve for data processing, made of a \n",
    "succession of increasingly refined data filters -- the \"layers\".\n",
    "\n",
    "Here our network consists of a sequence of two densely-connected (also called \"fully-connected\") neural layers. \n",
    "The second (and last) layer is a 10-way \"softmax\" layer, which means it will return an array of 10 probability scores (summing to 1). Each \n",
    "score will be the probability that the current digit image belongs to one of our 10 digit classes.\n",
    "\n",
    "To make our network ready for training, we need to pick three more things:\n",
    "\n",
    "* A loss function: this is how the network will be able to measure how good a job it is doing on its training data, and thus how it will be able to steer itself in the right direction.\n",
    "* An optimizer: this is the mechanism through which the network will update itself based on the data it sees and its loss function.\n",
    "* Metrics to monitor during training and testing. Here we will only care about accuracy (the fraction of the images that were correctly \n",
    "classified).\n",
    "\n",
    "Before defining the elements above, we need to initialize the orca context:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Initializing orca context\n",
      "Current pyspark location is : /intern/spark/spark-2.4.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/__init__.py\n",
      "Start to getOrCreate SparkContext\n",
      "pyspark_submit_args is:  --driver-class-path /home/jinglei/analytics-zoo/zoo/target/analytics-zoo-bigdl_0.12.1-spark_2.4.3-0.10.0-SNAPSHOT-jar-with-dependencies.jar pyspark-shell \n",
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/home/jinglei/analytics-zoo/zoo/target/analytics-zoo-bigdl_0.12.1-spark_2.4.3-0.10.0-SNAPSHOT-jar-with-dependencies.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/intern/spark/spark-2.4.3-bin-hadoop2.7/jars/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2021-02-22 17:28:59 WARN  Utils:66 - Your hostname, intern01 resolves to a loopback address: 127.0.1.1; using 10.239.44.107 instead (on interface eno1)\n",
      "2021-02-22 17:28:59 WARN  Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "2021-02-22 17:29:00 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "cls.getname: com.intel.analytics.bigdl.python.api.Sample\n",
      "BigDLBasePickler registering: bigdl.util.common  Sample\n",
      "Successfully got a SparkContext\n",
      "cls.getname: com.intel.analytics.bigdl.python.api.EvaluatedResult\n",
      "BigDLBasePickler registering: bigdl.util.common  EvaluatedResult\n",
      "cls.getname: com.intel.analytics.bigdl.python.api.JTensor\n",
      "BigDLBasePickler registering: bigdl.util.common  JTensor\n",
      "cls.getname: com.intel.analytics.bigdl.python.api.JActivity\n",
      "BigDLBasePickler registering: bigdl.util.common  JActivity\n",
      "\n",
      "User settings:\n",
      "\n",
      "   KMP_AFFINITY=granularity=fine,compact,1,0\n",
      "   KMP_BLOCKTIME=0\n",
      "   KMP_SETTINGS=1\n",
      "   OMP_NUM_THREADS=1\n",
      "\n",
      "Effective settings:\n",
      "\n",
      "   KMP_ABORT_DELAY=0\n",
      "   KMP_ADAPTIVE_LOCK_PROPS='1,1024'\n",
      "   KMP_ALIGN_ALLOC=64\n",
      "   KMP_ALL_THREADPRIVATE=128\n",
      "   KMP_ATOMIC_MODE=2\n",
      "   KMP_BLOCKTIME=0\n",
      "   KMP_CPUINFO_FILE: value is not defined\n",
      "   KMP_DETERMINISTIC_REDUCTION=false\n",
      "   KMP_DEVICE_THREAD_LIMIT=2147483647\n",
      "   KMP_DISP_HAND_THREAD=false\n",
      "   KMP_DISP_NUM_BUFFERS=7\n",
      "   KMP_DUPLICATE_LIB_OK=false\n",
      "   KMP_FORCE_REDUCTION: value is not defined\n",
      "   KMP_FOREIGN_THREADS_THREADPRIVATE=true\n",
      "   KMP_FORKJOIN_BARRIER='2,2'\n",
      "   KMP_FORKJOIN_BARRIER_PATTERN='hyper,hyper'\n",
      "   KMP_FORKJOIN_FRAMES=true\n",
      "   KMP_FORKJOIN_FRAMES_MODE=3\n",
      "   KMP_GTID_MODE=3\n",
      "   KMP_HANDLE_SIGNALS=false\n",
      "   KMP_HOT_TEAMS_MAX_LEVEL=1\n",
      "   KMP_HOT_TEAMS_MODE=0\n",
      "   KMP_INIT_AT_FORK=true\n",
      "   KMP_INIT_WAIT=2048\n",
      "   KMP_ITT_PREPARE_DELAY=0\n",
      "   KMP_LIBRARY=throughput\n",
      "   KMP_LOCK_KIND=queuing\n",
      "   KMP_MALLOC_POOL_INCR=1M\n",
      "   KMP_NEXT_WAIT=1024\n",
      "   KMP_NUM_LOCKS_IN_BLOCK=1\n",
      "   KMP_PLAIN_BARRIER='2,2'\n",
      "   KMP_PLAIN_BARRIER_PATTERN='hyper,hyper'\n",
      "   KMP_REDUCTION_BARRIER='1,1'\n",
      "   KMP_REDUCTION_BARRIER_PATTERN='hyper,hyper'\n",
      "   KMP_SCHEDULE='static,balanced;guided,iterative'\n",
      "   KMP_SETTINGS=true\n",
      "   KMP_SPIN_BACKOFF_PARAMS='4096,100'\n",
      "   KMP_STACKOFFSET=64\n",
      "   KMP_STACKPAD=0\n",
      "   KMP_STACKSIZE=4M\n",
      "   KMP_STORAGE_MAP=false\n",
      "   KMP_TASKING=2\n",
      "   KMP_TASKLOOP_MIN_TASKS=0\n",
      "   KMP_TASK_STEALING_CONSTRAINT=1\n",
      "   KMP_TEAMS_THREAD_LIMIT=8\n",
      "   KMP_TOPOLOGY_METHOD=all\n",
      "   KMP_USER_LEVEL_MWAIT=false\n",
      "   KMP_VERSION=false\n",
      "   KMP_WARNINGS=true\n",
      "   OMP_AFFINITY_FORMAT='OMP: pid %P tid %T thread %n bound to OS proc set {%a}'\n",
      "   OMP_ALLOCATOR=omp_default_mem_alloc\n",
      "   OMP_CANCELLATION=false\n",
      "   OMP_DEFAULT_DEVICE=0\n",
      "   OMP_DISPLAY_AFFINITY=false\n",
      "   OMP_DISPLAY_ENV=false\n",
      "   OMP_DYNAMIC=false\n",
      "   OMP_MAX_ACTIVE_LEVELS=2147483647\n",
      "   OMP_MAX_TASK_PRIORITY=0\n",
      "   OMP_NESTED=false\n",
      "   OMP_NUM_THREADS='1'\n",
      "   OMP_PLACES: value is not defined\n",
      "   OMP_PROC_BIND='intel'\n",
      "   OMP_SCHEDULE='static'\n",
      "   OMP_STACKSIZE=4M\n",
      "   OMP_TARGET_OFFLOAD=DEFAULT\n",
      "   OMP_THREAD_LIMIT=2147483647\n",
      "   OMP_TOOL=enabled\n",
      "   OMP_TOOL_LIBRARIES: value is not defined\n",
      "   OMP_WAIT_POLICY=PASSIVE\n",
      "   KMP_AFFINITY='noverbose,warnings,respect,granularity=fine,compact,1,0'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from zoo.orca import init_orca_context, stop_orca_context\n",
    "from zoo.orca import OrcaContext\n",
    "\n",
    "# recommended to set it to True when running Analytics Zoo in Jupyter notebook. \n",
    "OrcaContext.log_output = True # (this will display terminal's stdout and stderr in the Jupyter notebook).\n",
    "\n",
    "cluster_mode = \"local\"\n",
    "\n",
    "if cluster_mode == \"local\":\n",
    "    init_orca_context(cores=1, memory=\"2g\")   # run in local mode\n",
    "elif cluster_mode == \"k8s\":\n",
    "    init_orca_context(cluster_mode=\"k8s\", num_nodes=2, cores=4) # run on K8s cluster\n",
    "elif cluster_mode == \"yarn\":\n",
    "    init_orca_context(\n",
    "        cluster_mode=\"yarn-client\", cores=4, num_nodes=2, memory=\"2g\",\n",
    "        driver_memory=\"10g\", driver_cores=1,\n",
    "        conf={\"spark.rpc.message.maxSize\": \"1024\",\n",
    "              \"spark.task.maxFailures\": \"1\",\n",
    "              \"spark.driver.extraJavaOptions\": \"-Dbigdl.failure.retryTimes=1\"})   # run on Hadoop YARN cluster"
   ]
  },
  {
   "source": [
    "Specify loss function, optimizer, metrics, as well as the batch size for training/testing (number of samples utilized in one iteration):"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "creating: createZooKerasAccuracy\n"
     ]
    }
   ],
   "source": [
    "from zoo.orca.learn.metrics import Accuracy\n",
    "\n",
    "criterion = nn.NLLLoss()                                # Loss function\n",
    "adam = torch.optim.Adam(model.parameters(), 0.001)      # Optimizer\n",
    "metrics=[Accuracy()]                                    # Metrics\n",
    "\n",
    "train_batch_size=320\n",
    "test_batch_size=320"
   ]
  },
  {
   "source": [
    "To load the data for training and evaluation, we can use Pytorch DataLoader. The data should be normalized before the training process, and the training data needs to be shuffled so that the model can converge faster."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "torch.manual_seed(0)\n",
    "dir='./dataset'\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(dir, train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size= train_batch_size, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(dir, train=False,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=test_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to train our network using Orca Pytorch Estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "h 5 33600/60160][Iteration 857][Wall Clock 79.345263617s] Trained 320.0 records in 0.089098658 seconds. Throughput is 3591.5244 records/second. Loss is 0.043108232. \n",
      "2021-02-22 17:31:42 INFO  DistriOptimizer$:427 - [Epoch 5 33920/60160][Iteration 858][Wall Clock 79.422836779s] Trained 320.0 records in 0.077573162 seconds. Throughput is 4125.1377 records/second. Loss is 0.045115583. \n",
      "2021-02-22 17:31:42 INFO  DistriOptimizer$:427 - [Epoch 5 34240/60160][Iteration 859][Wall Clock 79.499603934s] Trained 320.0 records in 0.076767155 seconds. Throughput is 4168.449 records/second. Loss is 0.10594587. \n",
      "2021-02-22 17:31:42 INFO  DistriOptimizer$:427 - [Epoch 5 34560/60160][Iteration 860][Wall Clock 79.596266913s] Trained 320.0 records in 0.096662979 seconds. Throughput is 3310.4712 records/second. Loss is 0.04381498. \n",
      "2021-02-22 17:31:42 INFO  DistriOptimizer$:427 - [Epoch 5 34880/60160][Iteration 861][Wall Clock 79.67212964s] Trained 320.0 records in 0.075862727 seconds. Throughput is 4218.145 records/second. Loss is 0.0236854. \n",
      "2021-02-22 17:31:43 INFO  DistriOptimizer$:427 - [Epoch 5 35200/60160][Iteration 862][Wall Clock 79.750935477s] Trained 320.0 records in 0.078805837 seconds. Throughput is 4060.613 records/second. Loss is 0.03425867. \n",
      "2021-02-22 17:31:43 INFO  DistriOptimizer$:427 - [Epoch 5 35520/60160][Iteration 863][Wall Clock 79.837739191s] Trained 320.0 records in 0.086803714 seconds. Throughput is 3686.4783 records/second. Loss is 0.031337433. \n",
      "2021-02-22 17:31:43 INFO  DistriOptimizer$:427 - [Epoch 5 35840/60160][Iteration 864][Wall Clock 79.915717827s] Trained 320.0 records in 0.077978636 seconds. Throughput is 4103.688 records/second. Loss is 0.050876815. \n",
      "2021-02-22 17:31:43 INFO  DistriOptimizer$:427 - [Epoch 5 36160/60160][Iteration 865][Wall Clock 79.993606489s] Trained 320.0 records in 0.077888662 seconds. Throughput is 4108.4287 records/second. Loss is 0.032107152. \n",
      "2021-02-22 17:31:43 INFO  DistriOptimizer$:427 - [Epoch 5 36480/60160][Iteration 866][Wall Clock 80.078531996s] Trained 320.0 records in 0.084925507 seconds. Throughput is 3768.008 records/second. Loss is 0.024831222. \n",
      "2021-02-22 17:31:43 INFO  DistriOptimizer$:427 - [Epoch 5 36800/60160][Iteration 867][Wall Clock 80.155956246s] Trained 320.0 records in 0.07742425 seconds. Throughput is 4133.072 records/second. Loss is 0.018979222. \n",
      "2021-02-22 17:31:43 INFO  DistriOptimizer$:427 - [Epoch 5 37120/60160][Iteration 868][Wall Clock 80.237116332s] Trained 320.0 records in 0.081160086 seconds. Throughput is 3942.825 records/second. Loss is 0.017465455. \n",
      "2021-02-22 17:31:43 INFO  DistriOptimizer$:427 - [Epoch 5 37440/60160][Iteration 869][Wall Clock 80.322154959s] Trained 320.0 records in 0.085038627 seconds. Throughput is 3762.9958 records/second. Loss is 0.033737257. \n",
      "2021-02-22 17:31:43 INFO  DistriOptimizer$:427 - [Epoch 5 37760/60160][Iteration 870][Wall Clock 80.402921683s] Trained 320.0 records in 0.080766724 seconds. Throughput is 3962.0278 records/second. Loss is 0.033978395. \n",
      "2021-02-22 17:31:43 INFO  DistriOptimizer$:427 - [Epoch 5 38080/60160][Iteration 871][Wall Clock 80.481500455s] Trained 320.0 records in 0.078578772 seconds. Throughput is 4072.3467 records/second. Loss is 0.033031773. \n",
      "2021-02-22 17:31:43 INFO  DistriOptimizer$:427 - [Epoch 5 38400/60160][Iteration 872][Wall Clock 80.56413155s] Trained 320.0 records in 0.082631095 seconds. Throughput is 3872.634 records/second. Loss is 0.061982006. \n",
      "2021-02-22 17:31:43 INFO  DistriOptimizer$:427 - [Epoch 5 38720/60160][Iteration 873][Wall Clock 80.640249007s] Trained 320.0 records in 0.076117457 seconds. Throughput is 4204.0293 records/second. Loss is 0.02729443. \n",
      "2021-02-22 17:31:44 INFO  DistriOptimizer$:427 - [Epoch 5 39040/60160][Iteration 874][Wall Clock 80.720552991s] Trained 320.0 records in 0.080303984 seconds. Throughput is 3984.8584 records/second. Loss is 0.050817948. \n",
      "2021-02-22 17:31:44 INFO  DistriOptimizer$:427 - [Epoch 5 39360/60160][Iteration 875][Wall Clock 80.801028038s] Trained 320.0 records in 0.080475047 seconds. Throughput is 3976.388 records/second. Loss is 0.021709487. \n",
      "2021-02-22 17:31:44 INFO  DistriOptimizer$:427 - [Epoch 5 39680/60160][Iteration 876][Wall Clock 80.877495249s] Trained 320.0 records in 0.076467211 seconds. Throughput is 4184.8003 records/second. Loss is 0.037000235. \n",
      "2021-02-22 17:31:44 INFO  DistriOptimizer$:427 - [Epoch 5 40000/60160][Iteration 877][Wall Clock 80.956406487s] Trained 320.0 records in 0.078911238 seconds. Throughput is 4055.1892 records/second. Loss is 0.044047803. \n",
      "2021-02-22 17:31:44 INFO  DistriOptimizer$:427 - [Epoch 5 40320/60160][Iteration 878][Wall Clock 81.042995546s] Trained 320.0 records in 0.086589059 seconds. Throughput is 3695.617 records/second. Loss is 0.03212001. \n",
      "2021-02-22 17:31:44 INFO  DistriOptimizer$:427 - [Epoch 5 40640/60160][Iteration 879][Wall Clock 81.124310051s] Trained 320.0 records in 0.081314505 seconds. Throughput is 3935.3374 records/second. Loss is 0.054339964. \n",
      "2021-02-22 17:31:44 INFO  DistriOptimizer$:427 - [Epoch 5 40960/60160][Iteration 880][Wall Clock 81.210979433s] Trained 320.0 records in 0.086669382 seconds. Throughput is 3692.192 records/second. Loss is 0.024090772. \n",
      "2021-02-22 17:31:44 INFO  DistriOptimizer$:427 - [Epoch 5 41280/60160][Iteration 881][Wall Clock 81.296524425s] Trained 320.0 records in 0.085544992 seconds. Throughput is 3740.7217 records/second. Loss is 0.03747434. \n",
      "2021-02-22 17:31:44 INFO  DistriOptimizer$:427 - [Epoch 5 41600/60160][Iteration 882][Wall Clock 81.373984761s] Trained 320.0 records in 0.077460336 seconds. Throughput is 4131.1465 records/second. Loss is 0.021977272. \n",
      "2021-02-22 17:31:44 INFO  DistriOptimizer$:427 - [Epoch 5 41920/60160][Iteration 883][Wall Clock 81.454278491s] Trained 320.0 records in 0.08029373 seconds. Throughput is 3985.3672 records/second. Loss is 0.044771377. \n",
      "2021-02-22 17:31:44 INFO  DistriOptimizer$:427 - [Epoch 5 42240/60160][Iteration 884][Wall Clock 81.553739644s] Trained 320.0 records in 0.099461153 seconds. Throughput is 3217.3364 records/second. Loss is 0.027420368. \n",
      "2021-02-22 17:31:44 INFO  DistriOptimizer$:427 - [Epoch 5 42560/60160][Iteration 885][Wall Clock 81.63166867s] Trained 320.0 records in 0.077929026 seconds. Throughput is 4106.3003 records/second. Loss is 0.029580388. \n",
      "2021-02-22 17:31:45 INFO  DistriOptimizer$:427 - [Epoch 5 42880/60160][Iteration 886][Wall Clock 81.71000497s] Trained 320.0 records in 0.0783363 seconds. Throughput is 4084.9517 records/second. Loss is 0.056820534. \n",
      "2021-02-22 17:31:45 INFO  DistriOptimizer$:427 - [Epoch 5 43200/60160][Iteration 887][Wall Clock 81.795243759s] Trained 320.0 records in 0.085238789 seconds. Throughput is 3754.1592 records/second. Loss is 0.02477063. \n",
      "2021-02-22 17:31:45 INFO  DistriOptimizer$:427 - [Epoch 5 43520/60160][Iteration 888][Wall Clock 81.872150755s] Trained 320.0 records in 0.076906996 seconds. Throughput is 4160.87 records/second. Loss is 0.028343445. \n",
      "2021-02-22 17:31:45 INFO  DistriOptimizer$:427 - [Epoch 5 43840/60160][Iteration 889][Wall Clock 81.952720227s] Trained 320.0 records in 0.080569472 seconds. Throughput is 3971.7278 records/second. Loss is 0.04000344. \n",
      "2021-02-22 17:31:45 INFO  DistriOptimizer$:427 - [Epoch 5 44160/60160][Iteration 890][Wall Clock 82.038474725s] Trained 320.0 records in 0.085754498 seconds. Throughput is 3731.5828 records/second. Loss is 0.030642629. \n",
      "2021-02-22 17:31:45 INFO  DistriOptimizer$:427 - [Epoch 5 44480/60160][Iteration 891][Wall Clock 82.122892706s] Trained 320.0 records in 0.084417981 seconds. Throughput is 3790.6614 records/second. Loss is 0.026592437. \n",
      "2021-02-22 17:31:45 INFO  DistriOptimizer$:427 - [Epoch 5 44800/60160][Iteration 892][Wall Clock 82.199156767s] Trained 320.0 records in 0.076264061 seconds. Throughput is 4195.9478 records/second. Loss is 0.064664975. \n",
      "2021-02-22 17:31:45 INFO  DistriOptimizer$:427 - [Epoch 5 45120/60160][Iteration 893][Wall Clock 82.28117218s] Trained 320.0 records in 0.082015413 seconds. Throughput is 3901.7058 records/second. Loss is 0.018182803. \n",
      "2021-02-22 17:31:45 INFO  DistriOptimizer$:427 - [Epoch 5 45440/60160][Iteration 894][Wall Clock 82.358410129s] Trained 320.0 records in 0.077237949 seconds. Throughput is 4143.041 records/second. Loss is 0.049412064. \n",
      "2021-02-22 17:31:45 INFO  DistriOptimizer$:427 - [Epoch 5 45760/60160][Iteration 895][Wall Clock 82.439399972s] Trained 320.0 records in 0.080989843 seconds. Throughput is 3951.1125 records/second. Loss is 0.032558016. \n",
      "2021-02-22 17:31:45 INFO  DistriOptimizer$:427 - [Epoch 5 46080/60160][Iteration 896][Wall Clock 82.525367206s] Trained 320.0 records in 0.085967234 seconds. Throughput is 3722.3484 records/second. Loss is 0.040484972. \n",
      "2021-02-22 17:31:45 INFO  DistriOptimizer$:427 - [Epoch 5 46400/60160][Iteration 897][Wall Clock 82.602967778s] Trained 320.0 records in 0.077600572 seconds. Throughput is 4123.681 records/second. Loss is 0.040804356. \n",
      "2021-02-22 17:31:45 INFO  DistriOptimizer$:427 - [Epoch 5 46720/60160][Iteration 898][Wall Clock 82.686404933s] Trained 320.0 records in 0.083437155 seconds. Throughput is 3835.222 records/second. Loss is 0.031353228. \n",
      "2021-02-22 17:31:46 INFO  DistriOptimizer$:427 - [Epoch 5 47040/60160][Iteration 899][Wall Clock 82.77415852s] Trained 320.0 records in 0.087753587 seconds. Throughput is 3646.5747 records/second. Loss is 0.035769496. \n",
      "2021-02-22 17:31:46 INFO  DistriOptimizer$:427 - [Epoch 5 47360/60160][Iteration 900][Wall Clock 82.853057008s] Trained 320.0 records in 0.078898488 seconds. Throughput is 4055.8445 records/second. Loss is 0.0575606. \n",
      "2021-02-22 17:31:46 INFO  DistriOptimizer$:427 - [Epoch 5 47680/60160][Iteration 901][Wall Clock 82.930472706s] Trained 320.0 records in 0.077415698 seconds. Throughput is 4133.529 records/second. Loss is 0.07813205. \n",
      "2021-02-22 17:31:46 INFO  DistriOptimizer$:427 - [Epoch 5 48000/60160][Iteration 902][Wall Clock 83.023655375s] Trained 320.0 records in 0.093182669 seconds. Throughput is 3434.115 records/second. Loss is 0.050833277. \n",
      "2021-02-22 17:31:46 INFO  DistriOptimizer$:427 - [Epoch 5 48320/60160][Iteration 903][Wall Clock 83.102945215s] Trained 320.0 records in 0.07928984 seconds. Throughput is 4035.8262 records/second. Loss is 0.020558644. \n",
      "2021-02-22 17:31:46 INFO  DistriOptimizer$:427 - [Epoch 5 48640/60160][Iteration 904][Wall Clock 83.197020298s] Trained 320.0 records in 0.094075083 seconds. Throughput is 3401.5383 records/second. Loss is 0.04129862. \n",
      "2021-02-22 17:31:46 INFO  DistriOptimizer$:427 - [Epoch 5 48960/60160][Iteration 905][Wall Clock 83.283514536s] Trained 320.0 records in 0.086494238 seconds. Throughput is 3699.6685 records/second. Loss is 0.059891768. \n",
      "2021-02-22 17:31:46 INFO  DistriOptimizer$:427 - [Epoch 5 49280/60160][Iteration 906][Wall Clock 83.368488514s] Trained 320.0 records in 0.084973978 seconds. Throughput is 3765.859 records/second. Loss is 0.018326048. \n",
      "2021-02-22 17:31:46 INFO  DistriOptimizer$:427 - [Epoch 5 49600/60160][Iteration 907][Wall Clock 83.447370233s] Trained 320.0 records in 0.078881719 seconds. Throughput is 4056.7068 records/second. Loss is 0.022722397. \n",
      "2021-02-22 17:31:46 INFO  DistriOptimizer$:427 - [Epoch 5 49920/60160][Iteration 908][Wall Clock 83.54301737s] Trained 320.0 records in 0.095647137 seconds. Throughput is 3345.6309 records/second. Loss is 0.025883596. \n",
      "2021-02-22 17:31:46 INFO  DistriOptimizer$:427 - [Epoch 5 50240/60160][Iteration 909][Wall Clock 83.624678684s] Trained 320.0 records in 0.081661314 seconds. Throughput is 3918.6243 records/second. Loss is 0.035951667. \n",
      "2021-02-22 17:31:47 INFO  DistriOptimizer$:427 - [Epoch 5 50560/60160][Iteration 910][Wall Clock 83.703302143s] Trained 320.0 records in 0.078623459 seconds. Throughput is 4070.032 records/second. Loss is 0.042210884. \n",
      "2021-02-22 17:31:47 INFO  DistriOptimizer$:427 - [Epoch 5 50880/60160][Iteration 911][Wall Clock 83.796880815s] Trained 320.0 records in 0.093578672 seconds. Throughput is 3419.5825 records/second. Loss is 0.07665752. \n",
      "2021-02-22 17:31:47 INFO  DistriOptimizer$:427 - [Epoch 5 51200/60160][Iteration 912][Wall Clock 83.879325681s] Trained 320.0 records in 0.082444866 seconds. Throughput is 3881.3816 records/second. Loss is 0.056239348. \n",
      "2021-02-22 17:31:47 INFO  DistriOptimizer$:427 - [Epoch 5 51520/60160][Iteration 913][Wall Clock 83.956419429s] Trained 320.0 records in 0.077093748 seconds. Throughput is 4150.7905 records/second. Loss is 0.0574313. \n",
      "2021-02-22 17:31:47 INFO  DistriOptimizer$:427 - [Epoch 5 51840/60160][Iteration 914][Wall Clock 84.040543106s] Trained 320.0 records in 0.084123677 seconds. Throughput is 3803.923 records/second. Loss is 0.049584035. \n",
      "2021-02-22 17:31:47 INFO  DistriOptimizer$:427 - [Epoch 5 52160/60160][Iteration 915][Wall Clock 84.117921846s] Trained 320.0 records in 0.07737874 seconds. Throughput is 4135.503 records/second. Loss is 0.04659774. \n",
      "2021-02-22 17:31:47 INFO  DistriOptimizer$:427 - [Epoch 5 52480/60160][Iteration 916][Wall Clock 84.196883439s] Trained 320.0 records in 0.078961593 seconds. Throughput is 4052.603 records/second. Loss is 0.06632619. \n",
      "2021-02-22 17:31:47 INFO  DistriOptimizer$:427 - [Epoch 5 52800/60160][Iteration 917][Wall Clock 84.281202623s] Trained 320.0 records in 0.084319184 seconds. Throughput is 3795.1033 records/second. Loss is 0.028160017. \n",
      "2021-02-22 17:31:47 INFO  DistriOptimizer$:427 - [Epoch 5 53120/60160][Iteration 918][Wall Clock 84.35938429s] Trained 320.0 records in 0.078181667 seconds. Throughput is 4093.031 records/second. Loss is 0.03137618. \n",
      "2021-02-22 17:31:47 INFO  DistriOptimizer$:427 - [Epoch 5 53440/60160][Iteration 919][Wall Clock 84.438909413s] Trained 320.0 records in 0.079525123 seconds. Throughput is 4023.8857 records/second. Loss is 0.026717996. \n",
      "2021-02-22 17:31:47 INFO  DistriOptimizer$:427 - [Epoch 5 53760/60160][Iteration 920][Wall Clock 84.528782502s] Trained 320.0 records in 0.089873089 seconds. Throughput is 3560.5764 records/second. Loss is 0.0656531. \n",
      "2021-02-22 17:31:47 INFO  DistriOptimizer$:427 - [Epoch 5 54080/60160][Iteration 921][Wall Clock 84.610458264s] Trained 320.0 records in 0.081675762 seconds. Throughput is 3917.9312 records/second. Loss is 0.04953016. \n",
      "2021-02-22 17:31:48 INFO  DistriOptimizer$:427 - [Epoch 5 54400/60160][Iteration 922][Wall Clock 84.690136235s] Trained 320.0 records in 0.079677971 seconds. Throughput is 4016.1665 records/second. Loss is 0.023985568. \n",
      "2021-02-22 17:31:48 INFO  DistriOptimizer$:427 - [Epoch 5 54720/60160][Iteration 923][Wall Clock 84.776617776s] Trained 320.0 records in 0.086481541 seconds. Throughput is 3700.2117 records/second. Loss is 0.035615407. \n",
      "2021-02-22 17:31:48 INFO  DistriOptimizer$:427 - [Epoch 5 55040/60160][Iteration 924][Wall Clock 84.864008262s] Trained 320.0 records in 0.087390486 seconds. Throughput is 3661.726 records/second. Loss is 0.02885137. \n",
      "2021-02-22 17:31:48 INFO  DistriOptimizer$:427 - [Epoch 5 55360/60160][Iteration 925][Wall Clock 84.944565593s] Trained 320.0 records in 0.080557331 seconds. Throughput is 3972.3262 records/second. Loss is 0.03453691. \n",
      "2021-02-22 17:31:48 INFO  DistriOptimizer$:427 - [Epoch 5 55680/60160][Iteration 926][Wall Clock 85.029545052s] Trained 320.0 records in 0.084979459 seconds. Throughput is 3765.616 records/second. Loss is 0.06206103. \n",
      "2021-02-22 17:31:48 INFO  DistriOptimizer$:427 - [Epoch 5 56000/60160][Iteration 927][Wall Clock 85.1067228s] Trained 320.0 records in 0.077177748 seconds. Throughput is 4146.273 records/second. Loss is 0.0698054. \n",
      "2021-02-22 17:31:48 INFO  DistriOptimizer$:427 - [Epoch 5 56320/60160][Iteration 928][Wall Clock 85.186730906s] Trained 320.0 records in 0.080008106 seconds. Throughput is 3999.5947 records/second. Loss is 0.029834177. \n",
      "2021-02-22 17:31:48 INFO  DistriOptimizer$:427 - [Epoch 5 56640/60160][Iteration 929][Wall Clock 85.271879387s] Trained 320.0 records in 0.085148481 seconds. Throughput is 3758.1409 records/second. Loss is 0.030351114. \n",
      "2021-02-22 17:31:48 INFO  DistriOptimizer$:427 - [Epoch 5 56960/60160][Iteration 930][Wall Clock 85.355031498s] Trained 320.0 records in 0.083152111 seconds. Throughput is 3848.3691 records/second. Loss is 0.07878386. \n",
      "2021-02-22 17:31:48 INFO  DistriOptimizer$:427 - [Epoch 5 57280/60160][Iteration 931][Wall Clock 85.433926525s] Trained 320.0 records in 0.078895027 seconds. Throughput is 4056.0225 records/second. Loss is 0.04886693. \n",
      "2021-02-22 17:31:48 INFO  DistriOptimizer$:427 - [Epoch 5 57600/60160][Iteration 932][Wall Clock 85.516560896s] Trained 320.0 records in 0.082634371 seconds. Throughput is 3872.4805 records/second. Loss is 0.0333407. \n",
      "2021-02-22 17:31:48 INFO  DistriOptimizer$:427 - [Epoch 5 57920/60160][Iteration 933][Wall Clock 85.597325224s] Trained 320.0 records in 0.080764328 seconds. Throughput is 3962.145 records/second. Loss is 0.027230252. \n",
      "2021-02-22 17:31:48 INFO  DistriOptimizer$:427 - [Epoch 5 58240/60160][Iteration 934][Wall Clock 85.675638338s] Trained 320.0 records in 0.078313114 seconds. Throughput is 4086.1611 records/second. Loss is 0.03848866. \n",
      "2021-02-22 17:31:49 INFO  DistriOptimizer$:427 - [Epoch 5 58560/60160][Iteration 935][Wall Clock 85.759594242s] Trained 320.0 records in 0.083955904 seconds. Throughput is 3811.5247 records/second. Loss is 0.06152063. \n",
      "2021-02-22 17:31:49 INFO  DistriOptimizer$:427 - [Epoch 5 58880/60160][Iteration 936][Wall Clock 85.835908327s] Trained 320.0 records in 0.076314085 seconds. Throughput is 4193.1973 records/second. Loss is 0.032923408. \n",
      "2021-02-22 17:31:49 INFO  DistriOptimizer$:427 - [Epoch 5 59200/60160][Iteration 937][Wall Clock 85.913411948s] Trained 320.0 records in 0.077503621 seconds. Throughput is 4128.8394 records/second. Loss is 0.07016848. \n",
      "2021-02-22 17:31:49 INFO  DistriOptimizer$:427 - [Epoch 5 59520/60160][Iteration 938][Wall Clock 85.992577896s] Trained 320.0 records in 0.079165948 seconds. Throughput is 4042.1418 records/second. Loss is 0.03392674. \n",
      "2021-02-22 17:31:49 INFO  DistriOptimizer$:427 - [Epoch 5 59840/60160][Iteration 939][Wall Clock 86.071633508s] Trained 320.0 records in 0.079055612 seconds. Throughput is 4047.7834 records/second. Loss is 0.029882258. \n",
      "2021-02-22 17:31:49 INFO  DistriOptimizer$:427 - [Epoch 5 60160/60160][Iteration 940][Wall Clock 86.124388062s] Trained 320.0 records in 0.052754554 seconds. Throughput is 6065.827 records/second. Loss is 0.07037376. \n",
      "2021-02-22 17:31:49 INFO  DistriOptimizer$:472 - [Epoch 5 60160/60160][Iteration 940][Wall Clock 86.124388062s] Epoch finished. Wall clock time is 87921.741495 ms\n",
      "2021-02-22 17:31:49 INFO  DistriOptimizer$:111 - [Epoch 5 60160/60160][Iteration 940][Wall Clock 86.124388062s] Validate model...\n",
      "[Stage 1893:>                                                       (0 + 1) / 1]Warn: jep.JepException: <class 'StopIteration'>\n",
      "\tat jep.Jep.exec(Native Method)\n",
      "\tat jep.Jep.exec(Jep.java:478)\n",
      "\tat com.intel.analytics.zoo.common.PythonInterpreter$$anonfun$1.apply$mcV$sp(PythonInterpreter.scala:108)\n",
      "\tat com.intel.analytics.zoo.common.PythonInterpreter$$anonfun$1.apply(PythonInterpreter.scala:107)\n",
      "\tat com.intel.analytics.zoo.common.PythonInterpreter$$anonfun$1.apply(PythonInterpreter.scala:107)\n",
      "\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n",
      "\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2021-02-22 17:31:51 INFO  DistriOptimizer$:177 - [Epoch 5 60160/60160][Iteration 940][Wall Clock 86.124388062s] validate model throughput is 5835.1914 records/second\n",
      "2021-02-22 17:31:51 INFO  DistriOptimizer$:180 - [Epoch 5 60160/60160][Iteration 940][Wall Clock 86.124388062s] Top1Accuracy is Accuracy(correct: 9791, count: 10000, accuracy: 0.9791)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<zoo.orca.learn.pytorch.estimator.PyTorchSparkEstimator at 0x7fdc00eea790>"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "from zoo.orca.learn.pytorch import Estimator \n",
    "from zoo.orca.learn.trigger import EveryEpoch \n",
    "\n",
    "est = Estimator.from_torch(model=model, optimizer=adam, loss=criterion, metrics=metrics)\n",
    "est.fit(data=train_loader, epochs=5, validation_data=test_loader, batch_size=train_batch_size, checkpoint_trigger=EveryEpoch())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"loss\" of the network over the training data is displayed during training. The training accuracy can be obtained by evaluating the model upon the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Stage 1897:>                                                       (0 + 1) / 1]Warn: jep.JepException: <class 'StopIteration'>\n",
      "\tat jep.Jep.exec(Native Method)\n",
      "\tat jep.Jep.exec(Jep.java:478)\n",
      "\tat com.intel.analytics.zoo.common.PythonInterpreter$$anonfun$1.apply$mcV$sp(PythonInterpreter.scala:108)\n",
      "\tat com.intel.analytics.zoo.common.PythonInterpreter$$anonfun$1.apply(PythonInterpreter.scala:107)\n",
      "\tat com.intel.analytics.zoo.common.PythonInterpreter$$anonfun$1.apply(PythonInterpreter.scala:107)\n",
      "\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n",
      "\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2021-02-22 17:32:11 INFO  DistriOptimizer$:1759 - Top1Accuracy is Accuracy(correct: 59548, count: 60000, accuracy: 0.9924666666666667)\n",
      "{'Top1Accuracy': 0.9924666881561279}\n"
     ]
    }
   ],
   "source": [
    "train_result = est.evaluate(data=train_loader, batch_size=train_batch_size)\n",
    "print(train_result)"
   ]
  },
  {
   "source": [
    "We can see that the training accuracy is 0.992 (i.e. 99.2%). Now let's check if our model performs well on the test set too:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Stage 1899:>                                                       (0 + 1) / 1]Warn: jep.JepException: <class 'StopIteration'>\n",
      "\tat jep.Jep.exec(Native Method)\n",
      "\tat jep.Jep.exec(Jep.java:478)\n",
      "\tat com.intel.analytics.zoo.common.PythonInterpreter$$anonfun$1.apply$mcV$sp(PythonInterpreter.scala:108)\n",
      "\tat com.intel.analytics.zoo.common.PythonInterpreter$$anonfun$1.apply(PythonInterpreter.scala:107)\n",
      "\tat com.intel.analytics.zoo.common.PythonInterpreter$$anonfun$1.apply(PythonInterpreter.scala:107)\n",
      "\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n",
      "\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2021-02-22 17:32:14 INFO  DistriOptimizer$:1759 - Top1Accuracy is Accuracy(correct: 9791, count: 10000, accuracy: 0.9791)\n",
      "{'Top1Accuracy': 0.9790999889373779}\n"
     ]
    }
   ],
   "source": [
    "test_result = est.evaluate(data=test_loader, batch_size=test_batch_size)\n",
    "print(test_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Our test set accuracy turns out to be 97.9% -- that's quite a bit lower than the training set accuracy. \n",
    "This gap between training accuracy and test accuracy is an example of \"overfitting\", \n",
    "the fact that machine learning models tend to perform worse on new data than on their training data. \n",
    "Overfitting will be a central topic in chapter 3.\n",
    "\n",
    "This concludes our very first example -- you just saw how we could build and a train a neural network to classify handwritten digits. In the next chapter, we will go in detail over every moving piece we just previewed, and clarify what is really going on behind the scenes. You will learn about \"tensors\", the data-storing objects going into the network, about tensor operations, which layers are made of, and about gradient descent, which allows our network to learn from its training examples.\n",
    "\n",
    "Note: you should call `stop_orca_context()` when the program finishes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Stopping orca context\n"
     ]
    }
   ],
   "source": [
    "stop_orca_context()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('tf2': conda)",
   "metadata": {
    "interpreter": {
     "hash": "5056d60bd69d51d1e2df40a6c6e7c71a949f2ed4ad3fa2ff8fce432a7c7c0fe5"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}